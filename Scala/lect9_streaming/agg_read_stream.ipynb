{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.log4j._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load function & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSink(df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\")) // раз в 10 секунд, а по умолчанию раз в 1 секунду\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createParquetSink: (df: org.apache.spark.sql.DataFrame, fileName: String)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createParquetSink(df: DataFrame, \n",
    "                      fileName: String) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", s\"/tmp/$fileName\")\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$fileName\")\n",
    "    //.trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_rate = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_rate = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "sdf_rate.printSchema\n",
    "// sdf_rate.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"airport-codes.csv\")\n",
    "\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n",
       "col_to_schema: (sdf_tmp: org.apache.spark.sql.DataFrame, col: String)org.apache.spark.sql.types.StructType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Нам нужно получить схему для JSON, когда JSON в ячейке в столбце col датафрейма\n",
    "def col_to_schema(sdf_tmp: DataFrame, col: String): org.apache.spark.sql.types.StructType = {\n",
    "\n",
    "    val row_data: String = sdf_tmp.select( col ).collect()(0)(0).toString\n",
    "    val schema = spark.read.json(\n",
    "        SparkSession.active.sparkContext.parallelize(List(row_data))\n",
    "    ).schema\n",
    "    schema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. example1: Console Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sink_console = org.apache.spark.sql.streaming.DataStreamWriter@50175bc2\n",
       "sq_console = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@662bd605\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@662bd605"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------+-----+\n",
      "|timestamp             |value|\n",
      "+----------------------+-----+\n",
      "|2023-03-12 16:15:45.71|0    |\n",
      "|2023-03-12 16:15:47.71|2    |\n",
      "|2023-03-12 16:15:46.71|1    |\n",
      "|2023-03-12 16:15:48.71|3    |\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sink_console = createConsoleSink(sdf_rate)\n",
    "val sq_console = sink_console.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. example2: Parquet Sink1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 16:17:01 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_01.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_01.parquet\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -rm -r /tmp/tmp_01.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink_pq = org.apache.spark.sql.streaming.DataStreamWriter@2177c0da\n",
       "sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1593eeac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1593eeac"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sink_pq = createParquetSink(sdf_rate, \"tmp_01.parquet\")\n",
    "val sq_pq = sink_pq.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 seconds\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Found 15 items\n",
       "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:17 /tmp/tmp_01.parquet/_spark_metadata\n",
       "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:17 /tmp/tmp_01.parquet/commits\n",
       "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 16:17 /tmp/tmp_01.parquet/metadata\n",
       "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:17 /tmp/tmp_01.parquet/offsets\n",
       "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 16:17 /tmp/tmp_01.parquet/part-00000-0441a012-ef95-4c13-93ef-7db3d9a7deda-c000.snappy.parquet\n",
       "-rw-r--r--   3 dinar.sadykov hdfs        392 2023-03-12 16:17 /tmp/tmp_01.parquet/part-00000-0b2187e5-1444-44ef-91aa-d715f86e155f-c000.snappy.parquet\n",
       "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 16:17 /tmp/tmp_01.parquet/part-00000-39f...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"hadoop fs -ls /tmp/tmp_01.parquet\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-03-12 16:17:03.705|0    |\n",
      "|2023-03-12 16:17:04.705|1    |\n",
      "|2023-03-12 16:17:05.705|2    |\n",
      "|2023-03-12 16:17:06.705|3    |\n",
      "|2023-03-12 16:17:07.705|4    |\n",
      "+-----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rates = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rates = spark\n",
    "    .read\n",
    "    .parquet(\"/tmp/tmp_01.parquet\")\n",
    "println(rates.count)\n",
    "rates.printSchema\n",
    "rates.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. example3: Parquet Sink1 + ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idents = Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(00A, 00AA, 00AK, 00AL, 00AR, 00AS, 00AZ, 00CA, 00CL, 00CN, 00CO, 00FA, 00FD, 00FL, 00GA, 00GE, 00HI, 00ID, 00IG, 00II, 00IL, 00IN, 00IS, 00KS, 00KY, 00LA, 00LL, 00LS, 00MD, 00MI, 00MN, 00MO, 00MT, 00N, 00NC, 00NJ, 00NK, 00NY, 00OH, 00OI, 00OK, 00OR, 00PA, 00PN, 00PS, 00S, 00SC, 00SD, 00TA, 00TE, 00TN, 00TS, 00TX, 00UT, 00VA, 00VI, 00W, 00WA, 00WI, 00WN, 00WV, 00WY, 00XS, 01A, 01AK, 01AL, 01AR, 01AZ, 01C, 01CA, 01CL, 01CN, 01CO, 01CT, 01FA, 01FD, 01FL, 01GA, 01GE, 01IA, 01ID, 01II, 01IL, 01IN, 01IS, 01J, 01K, 01KS, 01KY, 01LA, 01LL, 01LS, 01MA, 01MD, 01ME, 01MI, 01MN, 01MO, 01MT, 01NC, 01NE, 01NH, 01NJ, 01NM, 01NV, 01NY, 01OI, 01OK, 01OR, 01PA, 01PN, 01PS, 01SC, 01TA, 01TE, 01TN, 01TS, 01TX, 01U, 01UT, 01VA, 01WA, 01WI, 01WN, 01WT, 01WY, 01XA, 01XS, 02AK, 02..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idents = airports.select(\"ident\").limit(200).distinct.as[String].collect\n",
    "\n",
    "val ident_sdf_rate = sdf_rate.withColumn(\"ident\"\n",
    "                              , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                                  array(\n",
    "                                      idents.map(lit(_)):_*)\n",
    "                              )(0)) // берем первый элемент массива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 16:17:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_02.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_02.parquet\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -rm -r /tmp/tmp_02.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@325b44b8\n",
       "ident_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@60ea23f8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@60ea23f8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_sq_pq_sink = createParquetSink(ident_sdf_rate, \"tmp_02.parquet\")\n",
    "val ident_sq_pq = ident_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 seconds\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        257 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        257 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2560 2023-03-12 16:17 /tmp/tmp_02.parquet/_spark_metadata/9.compact\n",
      "Found 10 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 16:17 /tmp/tmp_02.parquet/commits/9\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 16:17 /tmp/tmp_02.parquet/metadata\n",
      "Found 11 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        409 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/10\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 16:17 /tmp/tmp_02.parquet/offsets/9\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        991 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-192cfc2b-465c-4e91-b47d-75b5bc42fef9-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-501df0ba-41b5-4c41-ab7a-85c4226f057f-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs          0 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-8c753e63-185b-4481-91cf-204ac5a4ccfc-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-a23dc80c-ffc2-4b77-a6af-7c6f082ac9f1-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-a578e5ac-8866-4a88-bdb3-93c23f544cfd-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-a6f42755-ecef-47ec-a66d-792cc6179033-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-b1961129-f12d-41dd-a072-701144715498-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        472 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-bc2ff94a-d51a-4d6c-aa8a-0c95f95191a9-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-db56254a-de62-4012-b75f-5497ecd8fc19-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-ecf9ef3f-de7f-4a4d-b506-82aa2d5b718b-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 16:17 /tmp/tmp_02.parquet/part-00000-f7c3a821-f141-427f-b08b-049dbb62934a-c000.snappy.parquet\n",
      "Found 1 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:17 /tmp/tmp_02.parquet/sources/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_02.parquet/*\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+-----+\n",
      "|timestamp             |value|ident|\n",
      "+----------------------+-----+-----+\n",
      "|2023-03-12 16:17:45.01|1    |00A  |\n",
      "+----------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident_pq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_pq = spark.read\n",
    "    .parquet(\"/tmp/tmp_02.parquet/part-00000-192cfc2b-465c-4e91-b47d-75b5bc42fef9-c000.snappy.parquet\")\n",
    "\n",
    "println(ident_pq.count)\n",
    "ident_pq.printSchema\n",
    "ident_pq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Работа с Kafka с помощь Static Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+-----+\n",
      "|timestamp             |value|ident|\n",
      "+----------------------+-----+-----+\n",
      "|2023-03-12 16:17:44.01|0    |01MD |\n",
      "|2023-03-12 16:17:46.01|2    |00NK |\n",
      "|2023-03-12 16:17:47.01|3    |00IL |\n",
      "|2023-03-12 16:17:48.01|4    |01MD |\n",
      "|2023-03-12 16:17:49.01|5    |00LS |\n",
      "+----------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident_pq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_pq = spark.read\n",
    "    .parquet(\"/tmp/tmp_02.parquet/\")\n",
    "\n",
    "println(ident_pq.count)\n",
    "ident_pq.printSchema\n",
    "ident_pq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// def writeKafka[T](topic: String, data: Dataset[T]): Unit = {\n",
    "//     val kafkaParams = Map(\n",
    "//         \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\"\n",
    "//     )\n",
    "    \n",
    "//     data\n",
    "//         .toJSON\n",
    "//         .withColumn(\"topic\", lit(topic))\n",
    "//         .write\n",
    "//         .format(\"kafka\")\n",
    "//         .options(kafkaParams)\n",
    "//         .save\n",
    "// }\n",
    "\n",
    "// writeKafka(\"test_topic0\", ident_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   310|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   311|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   312|2023-03-08 23:48:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0)\n",
       "sdf_kafka0 = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\"\n",
    "    )\n",
    "\n",
    "\n",
    "val sdf_kafka0 = spark.read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "sdf_kafka0.printSchema\n",
    "sdf_kafka0.show(3)\n",
    "sdf_kafka0.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из Kafka имеет несколько особенностей:\n",
    "- по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение\n",
    "- колонки `value` и `key` имеют тип `binary`, который необходимо десереализовать\n",
    "\n",
    "Чтобы прочитать только определенную часть топика, нам необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров `startingOffsets` , `endingOffsets`. Возьмем два случайных события:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|      topic|partition|offset|\n",
      "+-----------+---------+------+\n",
      "|test_topic0|        0|   322|\n",
      "|test_topic0|        0|   329|\n",
      "|test_topic0|        0|   364|\n",
      "|test_topic0|        0|   377|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// На основании этих событий подготовим параметры startingOffsets и endingOffsets\n",
    "\n",
    "sdf_kafka0\n",
    "    .sample(0.1)\n",
    "    .limit(10)\n",
    "    .select('topic, 'partition, 'offset)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   322|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   323|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   324|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   325|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   326|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   327|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   328|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   329|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   330|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   331|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   332|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   333|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   334|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   335|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   336|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   337|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   338|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   339|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   340|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   341|2023-03-08 23:48:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0, startingOffsets -> \" { \"test_topic0\": { \"0\": 322 } } \", endingOffsets -> \" { \"test_topic0\": { \"0\": 377 } }  \")\n",
       "sdf_kafka1 = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 322 } } \"\"\",\n",
    "        \"endingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 377 } }  \"\"\"//,\n",
    "        //\"failOnDataLoss\" -> \"false\"\n",
    "    )\n",
    "\n",
    "\n",
    "val sdf_kafka1 = spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "sdf_kafka1.printSchema\n",
    "sdf_kafka1.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию параметр `startingOffsets` имеет значение `earliest`, а `endingOffsets` - `latest`. Поэтому, когда мы не указывали эти параметры, Spark прочитал содержимое всего топика\n",
    "\n",
    "Чтобы получить наши данные, которые мы записали в топик, нам необходимо их десереализовать. В нашем случае достаточно использовать `.cast(\"string\")`, однако это работает не всегда, т.к. формат данных может быть произвольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2022-10-31T19:54:01.336+03:00\",\"value\":23,\"ident\":\"01CA\"}|\n",
      "|{\"timestamp\":\"2022-10-31T19:54:03.336+03:00\",\"value\":25,\"ident\":\"00IS\"}|\n",
      "|{\"timestamp\":\"2022-10-31T19:54:05.336+03:00\",\"value\":27,\"ident\":\"01U\"} |\n",
      "+-----------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+-----+\n",
      "|ident|timestamp                    |value|\n",
      "+-----+-----------------------------+-----+\n",
      "|01CA |2022-10-31T19:54:01.336+03:00|23   |\n",
      "|00IS |2022-10-31T19:54:03.336+03:00|25   |\n",
      "|01U  |2022-10-31T19:54:05.336+03:00|27   |\n",
      "+-----+-----------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_kafka1_json = [value: string]\n",
       "sdf_kafka1_json_parsed = [ident: string, timestamp: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, timestamp: string ... 1 more field]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_kafka1_json = sdf_kafka1\n",
    "    .select('value.cast(\"string\"))\n",
    "    .as[String]\n",
    "\n",
    "sdf_kafka1_json.show(3, false)\n",
    "\n",
    "val sdf_kafka1_json_parsed = spark.read\n",
    "    .json(sdf_kafka1_json)\n",
    "\n",
    "sdf_kafka1_json_parsed.printSchema\n",
    "sdf_kafka1_json_parsed.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Работа с Kafka с помощью Streaming DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании SDF из Kafka необходимо помнить, что:\n",
    "- `startingOffsets` по умолчанию имеет значение `latest`\n",
    "- `endingOffsets` использовать нельзя\n",
    "- количество сообщений за батч можно (и нужно) ограничить параметром `maxOffsetPerTrigger` (по умолчанию он не задан и первый батч будет содержать данные всего топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0, startingOffsets -> earliest, maxOffsetsPerTrigger -> 2)\n",
       "sdf_kafka2 = [key: binary, value: binary ... 5 more fields]\n",
       "sdf_kafka2_parsed = [value: string, topic: string ... 2 more fields]\n",
       "sdf_kafka2_sink = org.apache.spark.sql.streaming.DataStreamWriter@7e383d0b\n",
       "sdf_kafka2_sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3aa5fd55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3aa5fd55"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:38.336+03:00\",\"value\":0,\"ident\":\"00KS\"}|test_topic0|0        |310   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:40.336+03:00\",\"value\":2,\"ident\":\"00MO\"}|test_topic0|0        |311   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:42.336+03:00\",\"value\":4,\"ident\":\"01K\"} |test_topic0|0        |312   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:44.336+03:00\",\"value\":6,\"ident\":\"00AS\"}|test_topic0|0        |313   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:46.336+03:00\",\"value\":8,\"ident\":\"01ID\"} |test_topic0|0        |314   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:48.336+03:00\",\"value\":10,\"ident\":\"00II\"}|test_topic0|0        |315   |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:49.336+03:00\",\"value\":11,\"ident\":\"02II\"}|test_topic0|0        |316   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:51.336+03:00\",\"value\":13,\"ident\":\"01IA\"}|test_topic0|0        |317   |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:53.336+03:00\",\"value\":15,\"ident\":\"00S\"} |test_topic0|0        |318   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:55.336+03:00\",\"value\":17,\"ident\":\"02KT\"}|test_topic0|0        |319   |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\"earliest\"\"\",\n",
    "        \"maxOffsetsPerTrigger\" -> \"2\"\n",
    "    )\n",
    "\n",
    "val sdf_kafka2 = spark\n",
    "    .readStream // <- TOBE, AS IS = .read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "val sdf_kafka2_parsed = sdf_kafka2\n",
    "    .select('value.cast(\"string\")\n",
    "            , 'topic\n",
    "            , 'partition\n",
    "            , 'offset)\n",
    "\n",
    "val sdf_kafka2_sink = createConsoleSink(sdf_kafka2_parsed)\n",
    "\n",
    "val sdf_kafka2_sq = sdf_kafka2_sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы перезапустим этот стрим, он повторно прочитает все данные. Чтобы обеспечить сохранение состояния стрима после обработки каждого батча, нам необходимо добавить параметр `checkpointLocation` в опции `writeStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkWithCheckpoint: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSinkWithCheckpoint(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 16:20:16 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_03' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_03\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -rm -r /tmp/tmp_03\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdf_kafka2_sink_2 = org.apache.spark.sql.streaming.DataStreamWriter@5c401598\n",
       "sdf_kafka2_sq_2 = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7853a778\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7853a778"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:38.336+03:00\",\"value\":0,\"ident\":\"00KS\"}|test_topic0|0        |310   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:40.336+03:00\",\"value\":2,\"ident\":\"00MO\"}|test_topic0|0        |311   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:42.336+03:00\",\"value\":4,\"ident\":\"01K\"} |test_topic0|0        |312   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:44.336+03:00\",\"value\":6,\"ident\":\"00AS\"}|test_topic0|0        |313   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sdf_kafka2_sink_2 = createConsoleSinkWithCheckpoint(\"tmp_03\", sdf_kafka2_parsed)\n",
    "val sdf_kafka2_sq_2 = sdf_kafka2_sink_2.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n",
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "//After 10-20 second\n",
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:20 /tmp/tmp_03/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 16:20 /tmp/tmp_03/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:20 /tmp/tmp_03/offsets\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:20 /tmp/tmp_03/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_03/\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        432 2023-03-12 16:20 /tmp/tmp_03/offsets/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        432 2023-03-12 16:20 /tmp/tmp_03/offsets/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_03/offsets/\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1678627220950,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
      "{\"test_topic0\":{\"0\":314}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"hadoop fs -head /tmp/tmp_03/offsets/1/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Выводы:\n",
    "\n",
    "- Работать с Kafka можно как с использованием Static DF, так и с помощью Streaming DF\n",
    "- Чтобы стрим запоминал свое состояние после остановки, необходимо использовать checkpoint - директорию на HDFS (или локальной ФС), в которую будет сохранятся состояние стрима после каждого батча\n",
    "- Apache Kafka - распределенная система, обеспечивающая передачу потока данных в слабосвязанных системах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. laba04b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic1 = dinar.sadykov\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dinar.sadykov"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topic1 = \"dinar.sadykov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_price: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- p_date: integer (nullable = true)\n",
      "\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "|  category|    date|event_type|   item_id|item_price|       uid|  p_date|\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "|Kitchen...|20200426|       buy|Kitchen...|      1245|036ab2e...|20200426|\n",
      "|Enterta...|20200426|       buy|Enterta...|       252|032df29...|20200426|\n",
      "|Enterta...|20200206|       buy|Enterta...|      1815|0342da4...|20200206|\n",
      "|Kitchen...|20200206|       buy|Kitchen...|      3691|0355d72...|20200206|\n",
      "|Enterta...|20200206|       buy|Enterta...|      3432|038168f...|20200206|\n",
      "|Enterta...|20200306|       buy|Enterta...|      2211|039331b...|20200306|\n",
      "|Mobile-...|20200220|       buy|Mobile-...|      3472|034568f...|20200220|\n",
      "|  Clothing|20200220|       buy|Clothing-2|      3462|037ddba...|20200220|\n",
      "|Kitchen...|20200116|       buy|Kitchen...|       933|03525ab...|20200116|\n",
      "|Mobile-...|20200215|       buy|Mobile-...|      4321|0392f39...|20200215|\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_buy1 = [category: string, date: string ... 5 more fields]\n",
       "sdf_view1 = [category: string, date: string ... 5 more fields]\n",
       "sdf_visit1 = [category: string, date: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category: string, date: string ... 5 more fields]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_buy1 = spark\n",
    "    .read\n",
    "    .json(\"visits/buy\")\n",
    "    .filter(\"uid is not NULL\")\n",
    "\n",
    "val sdf_view1 = spark\n",
    "    .read\n",
    "    .json(\"visits/view\")\n",
    "    .filter(\"uid is not NULL\")\n",
    "\n",
    "val sdf_visit1 = sdf_buy1.union(sdf_view1)\n",
    "    .repartitionByRange(200, 'uid)\n",
    "    .cache()\n",
    "\n",
    "sdf_visit1.printSchema\n",
    "sdf_visit1.show(numRows = 10, truncate = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = false)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = false)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "buy_list = Array({\"uid\":\"036ab2e1-ac19-41bb-8db4-487e1202c860\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":1245}, {\"uid\":\"032df295-1a28-4900-b405-dee9e9271c98\",\"item_id\":\"Entertainment-equipment-0\",\"item_price\":252}, {\"uid\":\"0342da4b-6ef6-488e-b331-11777764e768\",\"item_id\":\"Entertainment-equipment-18\",\"item_price\":1815}, {\"uid\":\"0355d721-ad22-4730-b8de-27cf43795b28\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":3691}, {\"uid\":\"038168f6-3507-48bf-9593-708ad6db6309\",\"item_id\":\"Entertainment-equipment-17\",\"item_price\":3432}, {\"uid\":\"039331bf-0c05-4746-ba32-fa66f2475082\",\"item_id\":\"Entertainment-equipment-9\",\"item_price\":2211}, {\"uid\":\"034568f3-8b80-4740-9f31-371e3148de3a\",\"item_id\":\"Mobile-phones-5\",\"item_price\":3472}, {\"uid\":\"037ddbad-ad22-42ac-89f1-a0466fa24072\",\"item_id...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\"uid\":\"036ab2e1-ac19-41bb-8db4-487e1202c860\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":1245}, {\"uid\":\"032df295-1a28-4900-b405-dee9e9271c98\",\"item_id\":\"Entertainment-equipment-0\",\"item_price\":252}, {\"uid\":\"0342da4b-6ef6-488e-b331-11777764e768\",\"item_id\":\"Entertainment-equipment-18\",\"item_price\":1815}, {\"uid\":\"0355d721-ad22-4730-b8de-27cf43795b28\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":3691}, {\"uid\":\"038168f6-3507-48bf-9593-708ad6db6309\",\"item_id\":\"Entertainment-equipment-17\",\"item_price\":3432}, {\"uid\":\"039331bf-0c05-4746-ba32-fa66f2475082\",\"item_id\":\"Entertainment-equipment-9\",\"item_price\":2211}, {\"uid\":\"034568f3-8b80-4740-9f31-371e3148de3a\",\"item_id\":\"Mobile-phones-5\",\"item_price\":3472}, {\"uid\":\"037ddbad-ad22-42ac-89f1-a0466fa24072\",\"item_id..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "////////////////////////////////////////////////////////\n",
    "////////////////////////////////////////////////////////\n",
    "val buy_list = sdf_visit1\n",
    "    .filter(\" event_type == 'buy' \")\n",
    "    .select('uid, 'item_id, 'item_price)\n",
    "    .limit(200)\n",
    "    .distinct\n",
    "    .toJSON\n",
    "    .as[String].collect\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "val view_list = sdf_visit1\n",
    "    .filter(\" event_type == 'view' \")\n",
    "    .select('uid, 'item_id, 'item_price)\n",
    "    .limit(200)\n",
    "    .distinct\n",
    "    .toJSON\n",
    "    .as[String].collect\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "////////////////////////////////////////////////////////\n",
    "\n",
    "// timestamp -> timestamp\n",
    "// value -> offset\n",
    "// ident -> value\n",
    "// topic -> topic\n",
    "// randint -> partition\n",
    "\n",
    "val buy_rate = sdf_rate\n",
    "        .withColumn(\"ident\"\n",
    "                  , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                      array(\n",
    "                          buy_list.map(lit(_)):_*)\n",
    "                  )(0)) // берем первый элемент массива\n",
    "        .withColumn(\"topic\", lit(topic1))\n",
    "        .withColumnRenamed(\"value\",\"offset\")\n",
    "        .withColumn(\"partition\", (round( org.apache.spark.sql.functions.rand()*(10)+5,0)).cast(\"int\") )\n",
    "        .withColumnRenamed(\"ident\",\"value\")\n",
    "\n",
    "buy_rate.printSchema\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "val view_rate = sdf_rate\n",
    "        .withColumn(\"ident\"\n",
    "                  , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                      array(\n",
    "                          view_list.map(lit(_)):_*)\n",
    "                  )(0)) // берем первый элемент массива\n",
    "        .withColumn(\"topic\", lit(topic1))\n",
    "        .withColumnRenamed(\"value\",\"offset\")\n",
    "        .withColumn(\"partition\", (round( org.apache.spark.sql.functions.rand()*(10)+5,0)).cast(\"int\") )\n",
    "        .withColumnRenamed(\"ident\",\"value\")\n",
    "\n",
    "view_rate.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 16:21:46 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_04_b.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_04_b.parquet1678627306779\n"
     ]
    }
   ],
   "source": [
    "// val sink_tmp = createConsoleSink(view_rate)\n",
    "// val sq_tmp = sink_tmp.start\n",
    "\n",
    "//After 10-20 second\n",
    "// killAll()\n",
    "\n",
    "println(\"hadoop fs -rm -r /tmp/tmp_04_b.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 16:21:48 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_04_v.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_04_v.parquet\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -rm -r /tmp/tmp_04_v.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buy_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@764205a0\n",
       "buy_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@79305ac2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@79305ac2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val buy_sq_pq_sink = createParquetSink(buy_rate, \"tmp_04_b.parquet\")\n",
    "val buy_sq_pq = buy_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "view_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@25b6783d\n",
       "view_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5ebed4f2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5ebed4f2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val view_sq_pq_sink = createParquetSink(view_rate, \"tmp_04_v.parquet\")\n",
    "val view_sq_pq = view_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n",
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 second\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:22 /tmp/tmp_04_b.parquet/_spark_metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:22 /tmp/tmp_04_b.parquet/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 16:21 /tmp/tmp_04_b.parquet/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:21 /tmp/tmp_04_b.parquet/offsets\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2327 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-0451f711-156f-4e4f-b9ef-7d9f266ea6f2-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2353 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-0c72f24d-5e0e-4eca-a5ca-9f37847bf523-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2344 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-1c23f3bc-52cb-4b87-80c7-7010935e7b24-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2353 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-264c4fe0-58f2-4b5f-aafd-4aa798e35e7a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-536b2075-a47e-4477-8115-e57dfbc3e3da-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2344 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-61641e30-5577-4c05-a93d-cb71fdca6874-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2378 2023-03-12 16:22 /tmp/tmp_04_b.parquet/part-00000-93f8c549-431b-4a58-b960-75300020f92a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        642 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-b822af0e-296e-4128-9d5b-28f4f8a31054-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2326 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-c4f69f09-e388-41a7-a828-48ca6e2d90e1-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2290 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-ea8422ed-905d-41cc-ad20-dc84ee03f520-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2371 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-fc433921-5971-4f00-83ac-cee920035e3a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2362 2023-03-12 16:21 /tmp/tmp_04_b.parquet/part-00000-fdacf815-ac0d-4907-b8e2-6366e4bdbc71-c000.snappy.parquet\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:21 /tmp/tmp_04_b.parquet/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_04_b.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|           timestamp|offset|               value|        topic|partition|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|2023-03-12 16:21:...|     9|{\"uid\":\"03ee2a93-...|dinar.sadykov|       15|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_tmp = [timestamp: timestamp, offset: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_tmp = spark.read\n",
    "    .parquet(\"/tmp/tmp_04_b.parquet/part-00000-0451f711-156f-4e4f-b9ef-7d9f266ea6f2-c000.snappy.parquet\")\n",
    "sdf_tmp.count()\n",
    "sdf_tmp.printSchema\n",
    "sdf_tmp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:22 /tmp/tmp_04_v.parquet/_spark_metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:22 /tmp/tmp_04_v.parquet/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 16:21 /tmp/tmp_04_v.parquet/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:22 /tmp/tmp_04_v.parquet/offsets\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2362 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-0bcf1e15-af57-45ed-bce4-1e992e794a8a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2245 2023-03-12 16:22 /tmp/tmp_04_v.parquet/part-00000-22a21e8d-07a0-4137-8f0b-a429de4b6d64-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2297 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-41beddc0-5acf-4de5-a417-ccd4b91e97d0-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2290 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-49adcc02-b73f-436b-b101-0fdbb17ee099-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2407 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-4afee58f-55b0-42f7-b6b4-e18453118a9d-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-54fe71b9-7df2-42d9-b6d2-a0d6d857a530-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2362 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-69d5d57b-b8bb-47c2-8721-abe115b83034-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-6d46ba75-b46f-471f-a652-67a0499533ce-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        642 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-a83de017-b275-4d99-9673-f575162470aa-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2245 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-b5d89d83-1c12-4301-b8cb-0abe10114bb9-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2308 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-d5e23c65-d9b0-4b7a-9448-3a139b7e4345-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2301 2023-03-12 16:21 /tmp/tmp_04_v.parquet/part-00000-df440ad2-93b2-4361-9d2c-583c8ced16fa-c000.snappy.parquet\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 16:21 /tmp/tmp_04_v.parquet/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_04_v.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|           timestamp|offset|               value|        topic|partition|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|2023-03-12 16:21:...|     2|{\"uid\":\"03634065-...|dinar.sadykov|       12|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_tmp = [timestamp: timestamp, offset: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_tmp = spark.read\n",
    "    .parquet(\"/tmp/tmp_04_v.parquet/part-00000-0bcf1e15-af57-45ed-bce4-1e992e794a8a-c000.snappy.parquet\")\n",
    "sdf_tmp.count()\n",
    "sdf_tmp.printSchema\n",
    "sdf_tmp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_price: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n",
      "+--------------------+------+-------------+---------+--------------------+----------+--------------------+\n",
      "|           timestamp|offset|        topic|partition|             item_id|item_price|                 uid|\n",
      "+--------------------+------+-------------+---------+--------------------+----------+--------------------+\n",
      "|2023-03-12 16:21:...|     2|dinar.sadykov|       12|Household-applian...|      3785|03634065-874e-411...|\n",
      "+--------------------+------+-------------+---------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(item_id,StringType,true), StructField(item_price,LongType,true), StructField(uid,StringType,true))\n",
       "sdf_tmp_parse = [timestamp: timestamp, offset: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = col_to_schema(sdf_tmp, \"value\")\n",
    "\n",
    "// Зная схему - получаем JSON -> Columns\n",
    "val sdf_tmp_parse = sdf_tmp\n",
    "    .withColumn(\"root\", from_json('value, schema) )\n",
    "    .select( 'timestamp, 'offset, 'topic, 'partition, col(\"root.*\") )\n",
    "\n",
    "sdf_tmp_parse.printSchema\n",
    "sdf_tmp_parse.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
