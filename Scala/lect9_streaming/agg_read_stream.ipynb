{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "import org.apache.log4j._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load function & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSink(df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\")) // раз в 10 секунд, а по умолчанию раз в 1 секунду\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createParquetSink: (df: org.apache.spark.sql.DataFrame, fileName: String)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createParquetSink(df: DataFrame, \n",
    "                      fileName: String) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", s\"/tmp/$fileName\")\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$fileName\")\n",
    "    //.trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@10988909, rate, [timestamp#0, value#1L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "timestamp: timestamp, value: bigint\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@10988909, rate, [timestamp#0, value#1L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@10988909, rate, [timestamp#0, value#1L]\n",
      "\n",
      "== Physical Plan ==\n",
      "StreamingRelation rate, [timestamp#0, value#1L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_rate = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_rate = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "sdf_rate.printSchema\n",
    "sdf_rate.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"airport-codes.csv\")\n",
    "\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n",
       "col_to_schema: (sdf_tmp: org.apache.spark.sql.DataFrame, col: String)org.apache.spark.sql.types.StructType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Когда JSON в ячейке в столбце col датафрейма, функцией получаем схему начинки JSON \n",
    "def col_to_schema(sdf_tmp: DataFrame, col: String): org.apache.spark.sql.types.StructType = {\n",
    "\n",
    "    val row_data: String = sdf_tmp.select( col ).collect()(0)(0).toString\n",
    "    val schema = spark.read.json(\n",
    "        SparkSession.active.sparkContext.parallelize(List(row_data))\n",
    "    ).schema\n",
    "    schema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp_clear: (chkName: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Периодически нужно чистить Кэш\n",
    "def tmp_clear(chkName: String) = {\n",
    "    println(s\"hadoop fs -rm -r /tmp/$chkName\".!!)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. example1: Console Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sink_console = org.apache.spark.sql.streaming.DataStreamWriter@196f859a\n",
       "sq_console = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6fbcb059\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6fbcb059"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-03-12 20:19:14.037|0    |\n",
      "|2023-03-12 20:19:16.037|2    |\n",
      "|2023-03-12 20:19:18.037|4    |\n",
      "|2023-03-12 20:19:15.037|1    |\n",
      "|2023-03-12 20:19:17.037|3    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-03-12 20:19:19.037|5    |\n",
      "|2023-03-12 20:19:21.037|7    |\n",
      "|2023-03-12 20:19:23.037|9    |\n",
      "|2023-03-12 20:19:25.037|11   |\n",
      "|2023-03-12 20:19:27.037|13   |\n",
      "|2023-03-12 20:19:20.037|6    |\n",
      "|2023-03-12 20:19:22.037|8    |\n",
      "|2023-03-12 20:19:24.037|10   |\n",
      "|2023-03-12 20:19:26.037|12   |\n",
      "|2023-03-12 20:19:28.037|14   |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sink_console = createConsoleSink(sdf_rate)\n",
    "val sq_console = sink_console.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 seconds\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. example2: Parquet Sink1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:19:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_01.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_01.parquet1678641580325\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"tmp_01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink_pq = org.apache.spark.sql.streaming.DataStreamWriter@5e197325\n",
       "sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6a9865e4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6a9865e4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sink_pq = createParquetSink(sdf_rate, \"tmp_01.parquet\")\n",
    "val sq_pq = sink_pq.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 seconds\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:19 /tmp/tmp_01.parquet/_spark_metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:19 /tmp/tmp_01.parquet/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 20:19 /tmp/tmp_01.parquet/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:19 /tmp/tmp_01.parquet/offsets\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-047a4b6e-ce2c-4eb5-867a-63e4cb98fca8-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-04e01d79-31dc-4595-9116-b2c892aebc6f-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-0ebdc5b7-ba78-4222-997a-af5170910f53-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-1b32f0dd-7112-43eb-942c-ebd1d60aaa67-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-45c18c44-e917-4442-a9e6-cd462df19701-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-652bf90d-0348-4f9c-a0e6-8f57c14c780c-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-7371c64b-d74c-4de8-86f0-8457b422e4d0-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-73ad9cf5-107e-46e1-9d21-a055f3c5fb73-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-783119f3-b86d-45bd-96d4-fab23c0de8ba-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-96fa9f55-e6ea-4f7e-a752-3a7077168708-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-9d3f217c-d50f-44d1-94c2-af0a4287182e-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-c5d7d8f3-19a3-4dc1-9c47-616d690e7cfa-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-c9aa1bb3-9c96-46c0-b3ae-36b13d3d66c7-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-eaa91636-d241-421c-bdee-589564711e6a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        392 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-eeb8b602-9412-44a1-aeb0-8b143f628c1d-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        790 2023-03-12 20:19 /tmp/tmp_01.parquet/part-00000-f134634d-068a-4676-8b21-e5cae4bed929-c000.snappy.parquet\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:19 /tmp/tmp_01.parquet/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_01.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-03-12 20:19:40.852|0    |\n",
      "|2023-03-12 20:19:41.852|1    |\n",
      "|2023-03-12 20:19:42.852|2    |\n",
      "|2023-03-12 20:19:43.852|3    |\n",
      "|2023-03-12 20:19:44.852|4    |\n",
      "+-----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rates = [timestamp: timestamp, value: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rates = spark\n",
    "    .read\n",
    "    .parquet(\"/tmp/tmp_01.parquet\")\n",
    "println(rates.count)\n",
    "rates.printSchema\n",
    "rates.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. example3: Parquet Sink1 + ident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idents = Array(SSMK, SSML, SSMM, SSMN, SSMP, SSMQ, SSMR, SSMS, SSMT, SSMU)\n",
       "ident_sdf_rate = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Берем 10 случайных кодов Аэропорта\n",
    "val idents = airports.select(\"ident\").limit(10).distinct.as[String].collect\n",
    "\n",
    "val ident_sdf_rate = sdf_rate.withColumn(\"ident\"\n",
    "                              , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                                  array(\n",
    "                                      idents.map(lit(_)):_*)\n",
    "                              )(0)) // берем первый элемент массива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:20:42 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_02.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_02.parquet1678641642821\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"tmp_02.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@2fb0bc5f\n",
       "ident_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@79016317\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@79016317"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_sq_pq_sink = createParquetSink(ident_sdf_rate, \"tmp_02.parquet\")\n",
    "val ident_sq_pq = ident_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 seconds\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        257 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/10\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        258 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2561 2023-03-12 20:20 /tmp/tmp_02.parquet/_spark_metadata/9.compact\n",
      "Found 11 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/10\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         29 2023-03-12 20:20 /tmp/tmp_02.parquet/commits/9\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 20:20 /tmp/tmp_02.parquet/metadata\n",
      "Found 12 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/1\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        409 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/10\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        409 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/11\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/2\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/3\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/4\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/5\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/6\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/7\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/8\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        408 2023-03-12 20:20 /tmp/tmp_02.parquet/offsets/9\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        472 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-0fd00ac2-9478-4cd8-8f2d-edd4ea3b45c4-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-2cb150b3-a78e-4f03-9034-a7c467e46886-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-494dfad1-bdc2-40a2-b83f-c7ef7cbbb6c8-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-4b9430b4-7b13-483f-aa2c-273e7c607db8-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-5db52f30-44ae-495f-a405-27b62c7db208-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-729a27ed-52f6-4480-b412-2cb91a03e590-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-961d6138-2f2a-45e0-ab54-d393c4f8d935-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-b8d4387c-4f44-4032-916e-881436ca6922-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs          0 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-bff3d903-920d-426d-86a2-a47e2ae6ad2f-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-c538b04a-ecd7-4ec0-ae11-a1bd06c69a20-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-d1c4382d-3793-42f6-9f34-fcf73a69b33b-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       1000 2023-03-12 20:20 /tmp/tmp_02.parquet/part-00000-d4f345f6-4a9a-40a2-a29a-853a20dff8e8-c000.snappy.parquet\n",
      "Found 1 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:20 /tmp/tmp_02.parquet/sources/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_02.parquet/*\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident_pq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_pq = spark.read\n",
    "    .parquet(\"/tmp/tmp_02.parquet/part-00000-d4f345f6-4a9a-40a2-a29a-853a20dff8e8-c000.snappy.parquet\")\n",
    "\n",
    "println(ident_pq.count)\n",
    "ident_pq.printSchema\n",
    "ident_pq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:20:49.349|6    |SSML |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident_pq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_pq = spark.read\n",
    "    .parquet(\"/tmp/tmp_02.parquet/part-00000-d4f345f6-4a9a-40a2-a29a-853a20dff8e8-c000.snappy.parquet\")\n",
    "\n",
    "println(ident_pq.count)\n",
    "ident_pq.printSchema\n",
    "ident_pq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Работа с Kafka с помощь Static Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:20:43.349|0    |SSMR |\n",
      "|2023-03-12 20:20:44.349|1    |SSMU |\n",
      "|2023-03-12 20:20:45.349|2    |SSML |\n",
      "|2023-03-12 20:20:46.349|3    |SSMT |\n",
      "|2023-03-12 20:20:47.349|4    |SSMU |\n",
      "+-----------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident_pq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_pq = spark.read\n",
    "    .parquet(\"/tmp/tmp_02.parquet/\")\n",
    "\n",
    "println(ident_pq.count)\n",
    "ident_pq.printSchema\n",
    "ident_pq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// def writeKafka[T](topic: String, data: Dataset[T]): Unit = {\n",
    "//     val kafkaParams = Map(\n",
    "//         \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\"\n",
    "//     )\n",
    "    \n",
    "//     data\n",
    "//         .toJSON\n",
    "//         .withColumn(\"topic\", lit(topic))\n",
    "//         .write\n",
    "//         .format(\"kafka\")\n",
    "//         .options(kafkaParams)\n",
    "//         .save\n",
    "// }\n",
    "\n",
    "// writeKafka(\"test_topic0\", ident_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   310|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   311|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   312|2023-03-08 23:48:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0)\n",
       "sdf_kafka0 = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\"\n",
    "    )\n",
    "\n",
    "\n",
    "val sdf_kafka0 = spark.read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "sdf_kafka0.printSchema\n",
    "sdf_kafka0.show(3)\n",
    "sdf_kafka0.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из Kafka имеет несколько особенностей:\n",
    "- по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение\n",
    "- колонки `value` и `key` имеют тип `binary`, который необходимо десереализовать\n",
    "\n",
    "Чтобы прочитать только определенную часть топика, нам необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров `startingOffsets` , `endingOffsets`. Возьмем два случайных события:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|      topic|partition|offset|\n",
      "+-----------+---------+------+\n",
      "|test_topic0|        0|   313|\n",
      "|test_topic0|        0|   316|\n",
      "|test_topic0|        0|   319|\n",
      "|test_topic0|        0|   325|\n",
      "|test_topic0|        0|   331|\n",
      "|test_topic0|        0|   336|\n",
      "|test_topic0|        0|   358|\n",
      "|test_topic0|        0|   362|\n",
      "|test_topic0|        0|   377|\n",
      "|test_topic0|        0|   383|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// На основании этих событий подготовим параметры startingOffsets и endingOffsets\n",
    "\n",
    "sdf_kafka0\n",
    "    .sample(0.1)\n",
    "    .limit(10)\n",
    "    .select('topic, 'partition, 'offset)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   313|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   314|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   315|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   316|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   317| 2023-03-08 23:48:04|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   318| 2023-03-08 23:48:04|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   319| 2023-03-08 23:48:04|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   320|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   321|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   322|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   323|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   324|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   325|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   326|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   327|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   328|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   329|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   330|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   331|2023-03-08 23:48:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|test_topic0|        0|   332|2023-03-08 23:48:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0, startingOffsets -> \" { \"test_topic0\": { \"0\": 313 } } \", endingOffsets -> \" { \"test_topic0\": { \"0\": 383 } }  \")\n",
       "sdf_kafka1 = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 313 } } \"\"\",\n",
    "        \"endingOffsets\" -> \"\"\" { \"test_topic0\": { \"0\": 383 } }  \"\"\"//,\n",
    "        //\"failOnDataLoss\" -> \"false\"\n",
    "    )\n",
    "\n",
    "\n",
    "val sdf_kafka1 = spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "sdf_kafka1.printSchema\n",
    "sdf_kafka1.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию параметр `startingOffsets` имеет значение `earliest`, а `endingOffsets` - `latest`. Поэтому, когда мы не указывали эти параметры, Spark прочитал содержимое всего топика\n",
    "\n",
    "Чтобы получить наши данные, которые мы записали в топик, нам необходимо их десереализовать. В нашем случае достаточно использовать `.cast(\"string\")`, однако это работает не всегда, т.к. формат данных может быть произвольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:44.336+03:00\",\"value\":6,\"ident\":\"00AS\"} |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:46.336+03:00\",\"value\":8,\"ident\":\"01ID\"} |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:48.336+03:00\",\"value\":10,\"ident\":\"00II\"}|\n",
      "+-----------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+-----+\n",
      "|ident|timestamp                    |value|\n",
      "+-----+-----------------------------+-----+\n",
      "|00AS |2022-10-31T19:53:44.336+03:00|6    |\n",
      "|01ID |2022-10-31T19:53:46.336+03:00|8    |\n",
      "|00II |2022-10-31T19:53:48.336+03:00|10   |\n",
      "+-----+-----------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_kafka1_json = [value: string]\n",
       "sdf_kafka1_json_parsed = [ident: string, timestamp: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, timestamp: string ... 1 more field]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_kafka1_json = sdf_kafka1\n",
    "    .select('value.cast(\"string\"))\n",
    "    .as[String]\n",
    "\n",
    "sdf_kafka1_json.show(3, false)\n",
    "\n",
    "val sdf_kafka1_json_parsed = spark.read\n",
    "    .json(sdf_kafka1_json)\n",
    "\n",
    "sdf_kafka1_json_parsed.printSchema\n",
    "sdf_kafka1_json_parsed.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Работа с Kafka с помощью Streaming DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании SDF из Kafka необходимо помнить, что:\n",
    "- `startingOffsets` по умолчанию имеет значение `latest`\n",
    "- `endingOffsets` использовать нельзя\n",
    "- количество сообщений за батч можно (и нужно) ограничить параметром `maxOffsetPerTrigger` (по умолчанию он не задан и первый батч будет содержать данные всего топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic0, startingOffsets -> earliest, maxOffsetsPerTrigger -> 2)\n",
       "sdf_kafka2 = [key: binary, value: binary ... 5 more fields]\n",
       "sdf_kafka2_parsed = [value: string, topic: string ... 2 more fields]\n",
       "sdf_kafka2_sink = org.apache.spark.sql.streaming.DataStreamWriter@15a76d13\n",
       "sdf_kafka2_sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1edef287\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1edef287"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:38.336+03:00\",\"value\":0,\"ident\":\"00KS\"}|test_topic0|0        |310   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:40.336+03:00\",\"value\":2,\"ident\":\"00MO\"}|test_topic0|0        |311   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:42.336+03:00\",\"value\":4,\"ident\":\"01K\"} |test_topic0|0        |312   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:44.336+03:00\",\"value\":6,\"ident\":\"00AS\"}|test_topic0|0        |313   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                  |topic      |partition|offset|\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:46.336+03:00\",\"value\":8,\"ident\":\"01ID\"} |test_topic0|0        |314   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:48.336+03:00\",\"value\":10,\"ident\":\"00II\"}|test_topic0|0        |315   |\n",
      "+-----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic0\",\n",
    "        \"startingOffsets\" -> \"\"\"earliest\"\"\",\n",
    "        \"maxOffsetsPerTrigger\" -> \"2\"\n",
    "    )\n",
    "\n",
    "val sdf_kafka2 = spark\n",
    "    .readStream // <- TOBE, AS IS = .read\n",
    "    .format(\"kafka\")\n",
    "    .options(kafkaParams)\n",
    "    .load\n",
    "\n",
    "val sdf_kafka2_parsed = sdf_kafka2\n",
    "    .select('value.cast(\"string\")\n",
    "            , 'topic\n",
    "            , 'partition\n",
    "            , 'offset)\n",
    "\n",
    "val sdf_kafka2_sink = createConsoleSink(sdf_kafka2_parsed)\n",
    "\n",
    "val sdf_kafka2_sq = sdf_kafka2_sink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы перезапустим этот стрим, он повторно прочитает все данные. Чтобы обеспечить сохранение состояния стрима после обработки каждого батча, нам необходимо добавить параметр `checkpointLocation` в опции `writeStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkWithCheckpoint: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSinkWithCheckpoint(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:22:32 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_03' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_031678641752333\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"tmp_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdf_kafka2_sink_2 = org.apache.spark.sql.streaming.DataStreamWriter@5a8eba8d\n",
       "sdf_kafka2_sq_2 = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@12bf6387\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@12bf6387"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:38.336+03:00\",\"value\":0,\"ident\":\"00KS\"}|test_topic0|0        |310   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:40.336+03:00\",\"value\":2,\"ident\":\"00MO\"}|test_topic0|0        |311   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                 |topic      |partition|offset|\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2022-10-31T19:53:42.336+03:00\",\"value\":4,\"ident\":\"01K\"} |test_topic0|0        |312   |\n",
      "|{\"timestamp\":\"2022-10-31T19:53:44.336+03:00\",\"value\":6,\"ident\":\"00AS\"}|test_topic0|0        |313   |\n",
      "+----------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sdf_kafka2_sink_2 = createConsoleSinkWithCheckpoint(\"tmp_03\", sdf_kafka2_parsed)\n",
    "val sdf_kafka2_sq_2 = sdf_kafka2_sink_2.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic0]]\n",
      "Stopped KafkaV2[Subscribe[test_topic0]]\n"
     ]
    }
   ],
   "source": [
    "//After 10-20 second\n",
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:22 /tmp/tmp_03/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 20:22 /tmp/tmp_03/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:22 /tmp/tmp_03/offsets\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:22 /tmp/tmp_03/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_03/\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        432 2023-03-12 20:22 /tmp/tmp_03/offsets/0\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        432 2023-03-12 20:22 /tmp/tmp_03/offsets/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_03/offsets/\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n",
      "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1678641760002,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
      "{\"test_topic0\":{\"0\":314}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -head /tmp/tmp_03/offsets/1/\".!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Выводы:\n",
    "\n",
    "- Работать с Kafka можно как с использованием Static DF, так и с помощью Streaming DF\n",
    "- Чтобы стрим запоминал свое состояние после остановки, необходимо использовать checkpoint - директорию на HDFS (или локальной ФС), в которую будет сохранятся состояние стрима после каждого батча\n",
    "- Apache Kafka - распределенная система, обеспечивающая передачу потока данных в слабосвязанных системах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. laba04b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic1 = dinar.sadykov\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dinar.sadykov"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topic1 = \"dinar.sadykov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_price: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- p_date: integer (nullable = true)\n",
      "\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "|  category|    date|event_type|   item_id|item_price|       uid|  p_date|\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "|Kitchen...|20200426|       buy|Kitchen...|      1245|036ab2e...|20200426|\n",
      "|Enterta...|20200426|       buy|Enterta...|       252|032df29...|20200426|\n",
      "|Enterta...|20200206|       buy|Enterta...|      1815|0342da4...|20200206|\n",
      "|Kitchen...|20200206|       buy|Kitchen...|      3691|0355d72...|20200206|\n",
      "|Enterta...|20200206|       buy|Enterta...|      3432|038168f...|20200206|\n",
      "|Enterta...|20200306|       buy|Enterta...|      2211|039331b...|20200306|\n",
      "|Mobile-...|20200220|       buy|Mobile-...|      3472|034568f...|20200220|\n",
      "|  Clothing|20200220|       buy|Clothing-2|      3462|037ddba...|20200220|\n",
      "|Kitchen...|20200116|       buy|Kitchen...|       933|03525ab...|20200116|\n",
      "|Mobile-...|20200215|       buy|Mobile-...|      4321|0392f39...|20200215|\n",
      "+----------+--------+----------+----------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_buy1 = [category: string, date: string ... 5 more fields]\n",
       "sdf_view1 = [category: string, date: string ... 5 more fields]\n",
       "sdf_visit1 = [category: string, date: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category: string, date: string ... 5 more fields]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_buy1 = spark\n",
    "    .read\n",
    "    .json(\"visits/buy\")\n",
    "    .filter(\"uid is not NULL\")\n",
    "\n",
    "val sdf_view1 = spark\n",
    "    .read\n",
    "    .json(\"visits/view\")\n",
    "    .filter(\"uid is not NULL\")\n",
    "\n",
    "val sdf_visit1 = sdf_buy1.union(sdf_view1)\n",
    "    .repartitionByRange(200, 'uid)\n",
    "    .cache()\n",
    "\n",
    "sdf_visit1.printSchema\n",
    "sdf_visit1.show(numRows = 10, truncate = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = false)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = false)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "buy_list = Array({\"uid\":\"03c5ffaa-8252-41ac-b3bc-ad01e153c9e9\",\"item_id\":\"Kitchen-appliances-8\",\"item_price\":4944}, {\"uid\":\"0512ba78-fa67-4de6-a73a-ef82080c565d\",\"item_id\":\"Cameras-3\",\"item_price\":4292}, {\"uid\":\"0502209d-9832-4645-9d83-ab34500ba12c\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":3253}, {\"uid\":\"0544a761-1ade-4438-9bf2-b889293dbd05\",\"item_id\":\"Household-furniture-8\",\"item_price\":3114}, {\"uid\":\"050707ba-9503-4f11-bdcf-77c92e582aaa\",\"item_id\":\"Cameras-17\",\"item_price\":1286}, {\"uid\":\"053e2c8d-3968-4dcb-a102-8ea3f8946a03\",\"item_id\":\"Computers-3\",\"item_price\":1298}, {\"uid\":\"03fb8ac3-2b5c-4dae-8b8b-5ee929e5c5a3\",\"item_id\":\"Shoes-8\",\"item_price\":4012}, {\"uid\":\"03b22e4e-358d-41c9-9a52-8985addedfb1\",\"item_id\":\"Clothing-17\",\"item_price\":3880}, {\"uid\":\"03f26d66-ce54-4...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\"uid\":\"03c5ffaa-8252-41ac-b3bc-ad01e153c9e9\",\"item_id\":\"Kitchen-appliances-8\",\"item_price\":4944}, {\"uid\":\"0512ba78-fa67-4de6-a73a-ef82080c565d\",\"item_id\":\"Cameras-3\",\"item_price\":4292}, {\"uid\":\"0502209d-9832-4645-9d83-ab34500ba12c\",\"item_id\":\"Kitchen-appliances-14\",\"item_price\":3253}, {\"uid\":\"0544a761-1ade-4438-9bf2-b889293dbd05\",\"item_id\":\"Household-furniture-8\",\"item_price\":3114}, {\"uid\":\"050707ba-9503-4f11-bdcf-77c92e582aaa\",\"item_id\":\"Cameras-17\",\"item_price\":1286}, {\"uid\":\"053e2c8d-3968-4dcb-a102-8ea3f8946a03\",\"item_id\":\"Computers-3\",\"item_price\":1298}, {\"uid\":\"03fb8ac3-2b5c-4dae-8b8b-5ee929e5c5a3\",\"item_id\":\"Shoes-8\",\"item_price\":4012}, {\"uid\":\"03b22e4e-358d-41c9-9a52-8985addedfb1\",\"item_id\":\"Clothing-17\",\"item_price\":3880}, {\"uid\":\"03f26d66-ce54-4..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "////////////////////////////////////////////////////////\n",
    "////////////////////////////////////////////////////////\n",
    "val buy_list = sdf_visit1\n",
    "    .filter(\" event_type == 'buy' \")\n",
    "    .select('uid, 'item_id, 'item_price)\n",
    "    .limit(200)\n",
    "    .distinct\n",
    "    .toJSON\n",
    "    .as[String].collect\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "val view_list = sdf_visit1\n",
    "    .filter(\" event_type == 'view' \")\n",
    "    .select('uid, 'item_id, 'item_price)\n",
    "    .limit(200)\n",
    "    .distinct\n",
    "    .toJSON\n",
    "    .as[String].collect\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "////////////////////////////////////////////////////////\n",
    "\n",
    "// timestamp -> timestamp\n",
    "// value -> offset\n",
    "// ident -> value\n",
    "// topic -> topic\n",
    "// randint -> partition\n",
    "\n",
    "val buy_rate = sdf_rate\n",
    "        .withColumn(\"ident\"\n",
    "                  , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                      array(\n",
    "                          buy_list.map(lit(_)):_*)\n",
    "                  )(0)) // берем первый элемент массива\n",
    "        .withColumn(\"topic\", lit(topic1))\n",
    "        .withColumnRenamed(\"value\",\"offset\")\n",
    "        .withColumn(\"partition\", (round( org.apache.spark.sql.functions.rand()*(10)+5,0)).cast(\"int\") )\n",
    "        .withColumnRenamed(\"ident\",\"value\")\n",
    "\n",
    "buy_rate.printSchema\n",
    "\n",
    "////////////////////////////////////////////////////////\n",
    "val view_rate = sdf_rate\n",
    "        .withColumn(\"ident\"\n",
    "                  , shuffle( // для каждой строки будет перемешивание вутри массива\n",
    "                      array(\n",
    "                          view_list.map(lit(_)):_*)\n",
    "                  )(0)) // берем первый элемент массива\n",
    "        .withColumn(\"topic\", lit(topic1))\n",
    "        .withColumnRenamed(\"value\",\"offset\")\n",
    "        .withColumn(\"partition\", (round( org.apache.spark.sql.functions.rand()*(10)+5,0)).cast(\"int\") )\n",
    "        .withColumnRenamed(\"ident\",\"value\")\n",
    "\n",
    "view_rate.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val sink_tmp = createConsoleSink(view_rate)\n",
    "// val sq_tmp = sink_tmp.start\n",
    "\n",
    "//After 10-20 second\n",
    "// killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:23:27 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_04_b.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_04_b.parquet1678641807358\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"tmp_04_b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:23:28 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/tmp_04_v.parquet' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/tmp_04_v.parquet1678641808983\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"tmp_04_v.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buy_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@2993d355\n",
       "buy_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6bcd8e18\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6bcd8e18"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val buy_sq_pq_sink = createParquetSink(buy_rate, \"tmp_04_b.parquet\")\n",
    "val buy_sq_pq = buy_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "view_sq_pq_sink = org.apache.spark.sql.streaming.DataStreamWriter@51d0b7e\n",
       "view_sq_pq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@732cf391\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@732cf391"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val view_sq_pq_sink = createParquetSink(view_rate, \"tmp_04_v.parquet\")\n",
    "val view_sq_pq = view_sq_pq_sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n",
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "// After 10-20 second\n",
    "killAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_b.parquet/_spark_metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_b.parquet/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 20:23 /tmp/tmp_04_b.parquet/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_b.parquet/offsets\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2263 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-043ab707-e180-4115-b52f-3ce58b35d773-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2362 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-0e0cd228-847b-4d23-a0e5-7ef969b1f527-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2344 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-293e97ea-882d-4827-aaab-3a949408a5a1-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2272 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-3029f663-4664-4445-bc00-d59daed3074b-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2362 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-3fa1e69f-b4bb-4e16-8c84-62b81e3a4971-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2335 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-45d5ed34-3871-435a-bc04-4a5e142211f6-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2272 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-5447b29a-c654-48aa-9fb4-3e4d58da05e5-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2237 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-60d85028-bbd7-453b-acdf-21726c8c138a-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2259 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-7095b910-19f8-4c9b-8c8c-aef7e8556874-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2344 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-72e0efdd-fb0e-4755-ac30-ac2c8561f919-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2263 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-7d97bdec-87e0-4aca-b744-14d7ef976efe-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2263 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-94c697e5-4306-43b4-a8d7-fcc9400ac005-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        642 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-a41d0401-9350-4a8b-9ede-fb8a2cc8c0f6-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2299 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-bdc80b67-19d0-4094-b03a-6635f4502b82-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2335 2023-03-12 20:23 /tmp/tmp_04_b.parquet/part-00000-be1f995d-b42a-4a84-a695-d9604d484177-c000.snappy.parquet\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_b.parquet/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_04_b.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|           timestamp|offset|               value|        topic|partition|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|2023-03-12 20:23:...|     2|{\"uid\":\"0507c191-...|dinar.sadykov|       13|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_tmp = [timestamp: timestamp, offset: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_tmp = spark.read\n",
    "    .parquet(\"/tmp/tmp_04_b.parquet/part-00000-5447b29a-c654-48aa-9fb4-3e4d58da05e5-c000.snappy.parquet\")\n",
    "sdf_tmp.count()\n",
    "sdf_tmp.printSchema\n",
    "sdf_tmp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 items\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_v.parquet/_spark_metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_v.parquet/commits\n",
      "-rw-r--r--   3 dinar.sadykov hdfs         45 2023-03-12 20:23 /tmp/tmp_04_v.parquet/metadata\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_v.parquet/offsets\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-026319e8-f764-4a5c-ab7e-e14ac90565d0-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2245 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-22410b63-a586-42ff-b92f-2a9e12d5ff13-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2290 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-352041ff-c2a2-4848-9fbf-08d5864a58d9-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2396 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-52fd0127-b6b6-4089-85eb-b88724a91bcc-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2218 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-5e4307ef-37ef-4019-9a5a-a6ac44d11c5b-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-8457bf43-c245-4edc-9b91-44f4110832c3-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2299 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-8e83dc6c-051c-4891-a67b-1cb93de941e9-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2263 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-96e545f8-e6fb-4df9-9ad7-9d5a029a96df-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2254 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-b5076cb1-6362-48c4-8301-53bf646e2f9f-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2218 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-b8aa2523-e9a8-4813-a718-118c30e5adf3-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2236 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-c2f31825-c780-4d4b-ac7c-445af30b75ae-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs        642 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-e4ff5c1d-3f0f-4b4d-9579-4be5de4798c7-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2238 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-e7775af7-0584-4043-91a5-1dd28b44af23-c000.snappy.parquet\n",
      "-rw-r--r--   3 dinar.sadykov hdfs       2308 2023-03-12 20:23 /tmp/tmp_04_v.parquet/part-00000-fb088b48-a203-40f9-817a-1581e4b2cee3-c000.snappy.parquet\n",
      "drwxr-xr-x   - dinar.sadykov hdfs          0 2023-03-12 20:23 /tmp/tmp_04_v.parquet/sources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"hadoop fs -ls /tmp/tmp_04_v.parquet\".!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      "\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|           timestamp|offset|               value|        topic|partition|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "|2023-03-12 20:23:...|     1|{\"uid\":\"0355d721-...|dinar.sadykov|       11|\n",
      "+--------------------+------+--------------------+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdf_tmp = [timestamp: timestamp, offset: bigint ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 3 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdf_tmp = spark.read\n",
    "    .parquet(\"/tmp/tmp_04_v.parquet/part-00000-5e4307ef-37ef-4019-9a5a-a6ac44d11c5b-c000.snappy.parquet\")\n",
    "sdf_tmp.count()\n",
    "sdf_tmp.printSchema\n",
    "sdf_tmp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_price: long (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n",
      "+--------------------+------+-------------+---------+-------+----------+--------------------+\n",
      "|           timestamp|offset|        topic|partition|item_id|item_price|                 uid|\n",
      "+--------------------+------+-------------+---------+-------+----------+--------------------+\n",
      "|2023-03-12 20:23:...|     1|dinar.sadykov|       11|Shoes-0|       418|0355d721-ad22-473...|\n",
      "+--------------------+------+-------------+---------+-------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(item_id,StringType,true), StructField(item_price,LongType,true), StructField(uid,StringType,true))\n",
       "sdf_tmp_parse = [timestamp: timestamp, offset: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, offset: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = col_to_schema(sdf_tmp, \"value\")\n",
    "\n",
    "// Зная схему - получаем JSON -> Columns\n",
    "val sdf_tmp_parse = sdf_tmp\n",
    "    .withColumn(\"root\", from_json('value, schema) )\n",
    "    .select( 'timestamp, 'offset, 'topic, 'partition, col(\"root.*\") )\n",
    "\n",
    "sdf_tmp_parse.printSchema\n",
    "sdf_tmp_parse.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Что такое stateful streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**stateful streaming** - это вид поточной обработки данных, при которой при обработке батча с данными используются данные из предыдущих батчей\n",
    "\n",
    "Все операции с использованием select, filter, withColumn (кроме операций с плавающими окнами) являются stateless. На практике это означает:\n",
    "- стрим не выполняет операций, требующих работы с данными из разных батчей\n",
    "- после обработки батча стрим \"забывает\" про него\n",
    "- высокую пропускную способность\n",
    "- небольшое количество файлов и общий объем чекпоинта\n",
    "- возможность вносить существенные правки в код стрима без пересоздания чекпоинта\n",
    "\n",
    "Если при обработке стрима используются такие методы, как `join()`, `groupBy()`, `dropDuplicates()` или функции над плавающими окнами, то:\n",
    "- в стриме должна быть колонка с временной меткой, на основе которой можно определить `watermark`\n",
    "- стрим будет работать медленней, чем вы ожидаете\n",
    "- в чекпоинте будет МНОГО файлов\n",
    "- при внесении изменений в код стрима с большой вероятностью придется пересоздавать чекпоинт\n",
    "\n",
    "Подготовим функции для управления стримами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "randomIdent: ()org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def randomIdent() = {\n",
    "    \n",
    "    val idents = airports.select('ident).limit(5).distinct.as[String].collect\n",
    "    val columnArray = idents.map( x => lit(x) )\n",
    "    val sparkArray = array(columnArray:_*)\n",
    "    val shuffledArray = shuffle(sparkArray)\n",
    "\n",
    "    shuffledArray(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink2: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSink2(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Удаление дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark позволяет удалять дубликаты данных в стриме. Это можно сделать двумя способами:\n",
    "- без использования `watermark`\n",
    "- с использованием `watermark`\n",
    "\n",
    "### Без использования watermark\n",
    "- Хеш каждого элемента будет сохраняться в чекпоинте\n",
    "- В стриме полностью исключаются дубликаты\n",
    "- Со временем начнется деградация стрима"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [timestamp#1391, value#1392L, shuffle([00A,00AA,00AK,00AL,00AR], Some(-5644576063220082580))[0] AS ident#1399]\n",
      "+- StreamingRelation rate, [timestamp#1391, value#1392L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdfWithDuplicates = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfWithDuplicates = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "\n",
    "sdfWithDuplicates.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "StreamingDeduplicate [ident#1413], state info [ checkpoint = <unknown>, runId = f8a5e5df-00ed-4376-aad0-7f7339556cf3, opId = 0, ver = 0, numPartitions = 200], 0\n",
      "+- Exchange hashpartitioning(ident#1413, 200)\n",
      "   +- *(1) Project [timestamp#1405, value#1406L, shuffle([SSMK,SSML,SSMM,SSMN,SSMP], Some(4977013574783073570))[0] AS ident#1413]\n",
      "      +- StreamingRelation rate, [timestamp#1405, value#1406L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sdfWithoutDuplicates = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfWithoutDuplicates = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .dropDuplicates(Seq(\"ident\"))\n",
    "\n",
    "sdfWithoutDuplicates.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:24:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state1_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state1_sdr1678641880960\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state1_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:24:42 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state2_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state2_sdr1678641882587\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state2_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7704181d"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:24:46.911|0    |00AL |\n",
      "|2023-03-12 20:24:48.911|2    |00AL |\n",
      "|2023-03-12 20:24:47.911|1    |00AK |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:24:49.911|3    |00AR |\n",
      "|2023-03-12 20:24:51.911|5    |00AA |\n",
      "|2023-03-12 20:24:53.911|7    |00AL |\n",
      "|2023-03-12 20:24:55.911|9    |00AR |\n",
      "|2023-03-12 20:24:57.911|11   |00AR |\n",
      "|2023-03-12 20:24:50.911|4    |00AK |\n",
      "|2023-03-12 20:24:52.911|6    |00AK |\n",
      "|2023-03-12 20:24:54.911|8    |00AK |\n",
      "|2023-03-12 20:24:56.911|10   |00AK |\n",
      "|2023-03-12 20:24:58.911|12   |00AK |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:24:59.911|13   |00AA |\n",
      "|2023-03-12 20:25:01.911|15   |00AK |\n",
      "|2023-03-12 20:25:03.911|17   |00A  |\n",
      "|2023-03-12 20:25:05.911|19   |00AL |\n",
      "|2023-03-12 20:25:07.911|21   |00AR |\n",
      "|2023-03-12 20:25:00.911|14   |00A  |\n",
      "|2023-03-12 20:25:02.911|16   |00AK |\n",
      "|2023-03-12 20:25:04.911|18   |00AR |\n",
      "|2023-03-12 20:25:06.911|20   |00AR |\n",
      "|2023-03-12 20:25:08.911|22   |00AR |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkWithCheckpoint(\"state1_sdr\", sdfWithDuplicates).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2824d477"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:24:48.456|1    |SSMN |\n",
      "|2023-03-12 20:24:47.456|0    |SSMK |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:24:56.456|9    |SSMP |\n",
      "|2023-03-12 20:24:53.456|6    |SSMM |\n",
      "|2023-03-12 20:24:51.456|4    |SSML |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkWithCheckpoint(\"state2_sdr\", sdfWithoutDuplicates).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n",
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С использованием watermark\n",
    "- Хеш старых событий удаляется из чекпоинта\n",
    "- Появление дубликатов возможно, если они приходят c задержкой N > watermark\n",
    "- Стрим не деградирует со временем\n",
    "- Колонка, по которой делается `watermark`, должна быть включена в `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdfWithoutDuplicates_w = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfWithoutDuplicates_w = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .dropDuplicates(Seq(\"ident\", \"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:25:29 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state3_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state3_sdr1678641929380\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state3_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@202a0e18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:25:30.886|1    |00A  |\n",
      "|2023-03-12 20:25:31.886|2    |00AR |\n",
      "|2023-03-12 20:25:29.886|0    |00AR |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:25:32.886|3    |00AK |\n",
      "|2023-03-12 20:25:34.886|5    |00AR |\n",
      "|2023-03-12 20:25:33.886|4    |00AA |\n",
      "|2023-03-12 20:25:35.886|6    |00A  |\n",
      "|2023-03-12 20:25:38.886|9    |00AL |\n",
      "|2023-03-12 20:25:37.886|8    |00A  |\n",
      "|2023-03-12 20:25:36.886|7    |00AR |\n",
      "+-----------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-03-12 20:25:43.886|14   |00AA |\n",
      "|2023-03-12 20:25:41.886|12   |00AL |\n",
      "|2023-03-12 20:25:42.886|13   |00A  |\n",
      "|2023-03-12 20:25:46.886|17   |00A  |\n",
      "|2023-03-12 20:25:39.886|10   |00A  |\n",
      "|2023-03-12 20:25:40.886|11   |00AR |\n",
      "|2023-03-12 20:25:48.886|19   |00AR |\n",
      "|2023-03-12 20:25:45.886|16   |00AK |\n",
      "|2023-03-12 20:25:44.886|15   |00AK |\n",
      "|2023-03-12 20:25:47.886|18   |00AK |\n",
      "+-----------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkWithCheckpoint(\"state3_sdr\", sdfWithoutDuplicates_w).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в нашем стриме колонка `timestamp` имеет уникальное значение для каждой строки, то фактически дедупликация здесь работать не будет. Поэтому, дедупликация применима только для следующих случаев:\n",
    "- вы хотите избавиться от полностью идентичных событий, у которых ключ И метка времени одинаковые\n",
    "- вы делаете округление колонки с меткой времени\n",
    "\n",
    "Рассмотрим второй вариант подробнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "round: (thisCol: org.apache.spark.sql.Column, value: Int)org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def round(thisCol: Column, value: Int) = { \n",
    "    ((thisCol.cast(\"long\") / value).cast(\"long\") * value).cast(\"timestamp\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sdfWithoutDuplicates2 = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sdfWithoutDuplicates2 = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"timestamp\", round(col(\"timestamp\"), 60*10))\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .dropDuplicates(Seq(\"ident\", \"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sq_state4 = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@15462415\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@15462415"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------------------+-----+-----+\n",
      "|timestamp          |value|ident|\n",
      "+-------------------+-----+-----+\n",
      "|2023-03-12 19:40:00|2241 |SSMM |\n",
      "|2023-03-12 19:40:00|2240 |SSML |\n",
      "|2023-03-12 19:40:00|2238 |SSMK |\n",
      "|2023-03-12 19:40:00|2245 |SSMN |\n",
      "|2023-03-12 19:40:00|2242 |SSMP |\n",
      "+-------------------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-------------------+-----+-----+\n",
      "|timestamp          |value|ident|\n",
      "+-------------------+-----+-----+\n",
      "|2023-03-12 20:00:00|3381 |SSMP |\n",
      "|2023-03-12 19:50:00|2791 |SSMK |\n",
      "|2023-03-12 20:10:00|3979 |SSMP |\n",
      "|2023-03-12 19:50:00|2779 |SSMM |\n",
      "|2023-03-12 20:20:00|4579 |SSML |\n",
      "|2023-03-12 20:20:00|4587 |SSMP |\n",
      "|2023-03-12 19:50:00|2783 |SSMN |\n",
      "|2023-03-12 20:10:00|3981 |SSMN |\n",
      "|2023-03-12 20:20:00|4584 |SSMM |\n",
      "|2023-03-12 20:00:00|3412 |SSMM |\n",
      "|2023-03-12 20:10:00|3978 |SSMM |\n",
      "|2023-03-12 19:50:00|2788 |SSML |\n",
      "|2023-03-12 20:10:00|3993 |SSMK |\n",
      "|2023-03-12 20:00:00|3385 |SSMK |\n",
      "|2023-03-12 20:00:00|3379 |SSML |\n",
      "|2023-03-12 20:20:00|4592 |SSMK |\n",
      "|2023-03-12 19:50:00|2780 |SSMP |\n",
      "|2023-03-12 20:00:00|3389 |SSMN |\n",
      "|2023-03-12 20:10:00|3980 |SSML |\n",
      "|2023-03-12 20:20:00|4578 |SSMN |\n",
      "+-------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sq_state4 = createConsoleSinkWithCheckpoint(\"state4_sdr\", sdfWithoutDuplicates2).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\" : \"a836a40d-1e52-4aa4-ac0d-dd799cdefbb2\",\n",
      "  \"runId\" : \"ea6dce69-ed3d-49fa-9c10-bba4fc98e295\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-03-12T17:26:10.000Z\",\n",
      "  \"batchId\" : 15,\n",
      "  \"numInputRows\" : 2700,\n",
      "  \"inputRowsPerSecond\" : 320.3987184051264,\n",
      "  \"processedRowsPerSecond\" : 752.2986904430203,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 3491,\n",
      "    \"getBatch\" : 0,\n",
      "    \"getEndOffset\" : 0,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"setOffsetRange\" : 0,\n",
      "    \"triggerExecution\" : 3589,\n",
      "    \"walCommit\" : 30\n",
      "  },\n",
      "  \"eventTime\" : {\n",
      "    \"avg\" : \"2023-03-12T16:58:47.999Z\",\n",
      "    \"max\" : \"2023-03-12T17:20:00.000Z\",\n",
      "    \"min\" : \"2023-03-12T16:40:00.000Z\",\n",
      "    \"watermark\" : \"2023-03-12T16:30:00.000Z\"\n",
      "  },\n",
      "  \"stateOperators\" : [ {\n",
      "    \"numRowsTotal\" : 30,\n",
      "    \"numRowsUpdated\" : 20,\n",
      "    \"memoryUsedBytes\" : 89079,\n",
      "    \"customMetrics\" : {\n",
      "      \"loadedMapCacheHitCount\" : 200,\n",
      "      \"loadedMapCacheMissCount\" : 200,\n",
      "      \"stateOnCurrentVersionSizeBytes\" : 26359\n",
      "    }\n",
      "  } ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\",\n",
      "    \"startOffset\" : 2247,\n",
      "    \"endOffset\" : 4947,\n",
      "    \"numInputRows\" : 2700,\n",
      "    \"inputRowsPerSecond\" : 320.3987184051264,\n",
      "    \"processedRowsPerSecond\" : 752.2986904430203\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@4a8ecea4\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(sq_state4.lastProgress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет удалять дубликаты из стрима\n",
    "- Для стабильной работы требуется использовать `watermark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Агрегаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При построении агрегатов на стриме важно задать правильный `outputMode`, который может иметь три значения:\n",
    "- `append`\n",
    "- `update`\n",
    "- `complete`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkMode: (chkName: String, mode: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createConsoleSinkMode(chkName: String, mode: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .outputMode(mode)\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[ident#1399], functions=[count(1)])\n",
      "+- StateStoreSave [ident#1399], state info [ checkpoint = <unknown>, runId = ed8dd955-367b-4333-8d59-44e4cbede365, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[ident#1399], functions=[merge_count(1)])\n",
      "      +- StateStoreRestore [ident#1399], state info [ checkpoint = <unknown>, runId = ed8dd955-367b-4333-8d59-44e4cbede365, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[ident#1399], functions=[merge_count(1)])\n",
      "            +- Exchange hashpartitioning(ident#1399, 200)\n",
      "               +- *(1) HashAggregate(keys=[ident#1399], functions=[partial_count(1)])\n",
      "                  +- *(1) Project [shuffle([00A,00AA,00AK,00AL,00AR], Some(2189780357686438522))[0] AS ident#1399]\n",
      "                     +- StreamingRelation rate, [timestamp#1391, value#1392L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grouped = [ident: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, count: bigint]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grouped = sdfWithDuplicates.groupBy('ident).count\n",
    "\n",
    "grouped.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:26:24 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state4_mode_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state4_mode_sdr1678641984457\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state4_mode_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@d3fabae"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "|00AK |1    |\n",
      "|00A  |2    |\n",
      "|00AL |2    |\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "|00AK |3    |\n",
      "|00AA |3    |\n",
      "|00A  |4    |\n",
      "|00AR |3    |\n",
      "|00AL |2    |\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "|00AK |3    |\n",
      "|00AA |4    |\n",
      "|00A  |7    |\n",
      "|00AR |6    |\n",
      "|00AL |5    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkMode(\"state4_mode_sdr\", \"complete\", grouped).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:27:00 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state4_mode_sdr2' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state4_mode_sdr21678642020884\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state4_mode_sdr2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7efdc717"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "|00AA |2    |\n",
      "|00A  |2    |\n",
      "|00AR |2    |\n",
      "|00AL |2    |\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|ident|count|\n",
      "+-----+-----+\n",
      "|00AK |3    |\n",
      "|00AA |6    |\n",
      "|00A  |3    |\n",
      "|00AR |3    |\n",
      "|00AL |3    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkMode(\"state4_mode_sdr2\", \"update\", grouped).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "error:\n",
       "  unrecoverable error\n",
       "     while compiling: <console>\n",
       "        during phase: globalPhase=erasure, enteringPhase=posterasure\n",
       "     library version: version 2.11.12\n",
       "    compiler version: version 2.11.12\n",
       "  last tree to typer: TypeTree(class $iw)\n",
       "       tree position: line 7 of <console>\n",
       "            tree tpe: $iw\n",
       "              symbol: class $iw\n",
       "   symbol definition: class $iw extends Serializable (a ClassSymbol)\n",
       "      symbol package: $line203\n",
       "       symbol owners: class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $iw -> class $read\n",
       "           call site: package <root> in <none>\n",
       "<Cannot read source file>\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Агрегаты на плавающих окнах\n",
    "Плавающее окно позволяет сгруппировать события в окна определенного размера (по времени). При этом, поскольку каждое событие может находится одновременно в нескольких окнах, то общий размер агрегата существенно увеличивается\n",
    "\n",
    "Окно задается при создании агрегата с помощью функции `window` внутри `groupBy`. В параметрах указывается длина окна и расстояние между двумя точкой начала текущего и следующего окна.\n",
    "\n",
    "<img align=\"center\" width=\"1000\" height=\"1000\" src=\"https://spark.apache.org/docs/latest/img/structured-streaming-window.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим с использованием `window` и `watermark` в `update` режиме. В данном случае watermark позволяет игнорировать события, которые приходят с запозданием (`latency` > `max_event_timestamp` + `watermark_value`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:27:46 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state10_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state10_sdr1678642066150\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state10_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+-----+\n",
      "|timestamp|value|ident|\n",
      "+---------+-----+-----+\n",
      "+---------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oldData = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3c8241b9"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+-----+--------+\n",
      "|timestamp          |value|ident   |\n",
      "+-------------------+-----+--------+\n",
      "|2023-03-11 00:00:00|0    |OLD DATA|\n",
      "|2023-03-11 00:00:00|2    |OLD DATA|\n",
      "|2023-03-11 00:00:00|1    |OLD DATA|\n",
      "+-------------------+-----+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+-----+--------+\n",
      "|timestamp          |value|ident   |\n",
      "+-------------------+-----+--------+\n",
      "|2023-03-11 00:00:00|3    |OLD DATA|\n",
      "|2023-03-11 00:00:00|5    |OLD DATA|\n",
      "|2023-03-11 00:00:00|7    |OLD DATA|\n",
      "|2023-03-11 00:00:00|9    |OLD DATA|\n",
      "|2023-03-11 00:00:00|11   |OLD DATA|\n",
      "|2023-03-11 00:00:00|4    |OLD DATA|\n",
      "|2023-03-11 00:00:00|6    |OLD DATA|\n",
      "|2023-03-11 00:00:00|8    |OLD DATA|\n",
      "|2023-03-11 00:00:00|10   |OLD DATA|\n",
      "|2023-03-11 00:00:00|12   |OLD DATA|\n",
      "+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val oldData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", lit(\"OLD DATA\"))\n",
    "    .withColumn(\"timestamp\", date_sub('timestamp, 1).cast(\"timestamp\"))\n",
    "\n",
    "createConsoleSinkMode(\"state10_sdr\", \"append\", oldData).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:28:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state5_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state5_sdr\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state5_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newData = [timestamp: timestamp, value: bigint ... 1 more field]\n",
       "oldData = [timestamp: date, value: bigint ... 1 more field]\n",
       "uData = [window: struct<start: timestamp, end: timestamp>, ident: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[window: struct<start: timestamp, end: timestamp>, ident: string ... 1 more field]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "\n",
    "val oldData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", lit(\"OLD DATA\"))\n",
    "    .withColumn(\"timestamp\", date_sub('timestamp, 1))\n",
    "\n",
    "val uData = newData\n",
    "    .union(oldData)\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\")\n",
    "    .groupBy(window($\"timestamp\", \"1 minutes\"), 'ident)\n",
    "    .count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sq_tmp = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@44ff7a42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@44ff7a42"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-----+-----+\n",
      "|window|ident|count|\n",
      "+------+-----+-----+\n",
      "+------+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----+\n",
      "|window                                    |ident   |count|\n",
      "+------------------------------------------+--------+-----+\n",
      "|[2023-03-11 00:00:00, 2023-03-11 00:01:00]|OLD DATA|5    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSMP    |1    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSMM    |2    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSML    |2    |\n",
      "+------------------------------------------+--------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----+\n",
      "|window                                    |ident   |count|\n",
      "+------------------------------------------+--------+-----+\n",
      "|[2023-03-11 00:00:00, 2023-03-11 00:01:00]|OLD DATA|15   |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSMP    |3    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSMM    |5    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSMK    |2    |\n",
      "|[2023-03-12 20:28:00, 2023-03-12 20:29:00]|SSML    |5    |\n",
      "+------------------------------------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sq_tmp = createConsoleSinkMode(\"state5_sdr\", \"complete\", uData).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\" : \"5dbc6731-5772-4312-89a9-38b21250b68c\",\n",
      "  \"runId\" : \"bbddb334-6388-494f-8e3a-26e0a2b6d706\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2023-03-12T17:28:20.000Z\",\n",
      "  \"batchId\" : 2,\n",
      "  \"numInputRows\" : 20,\n",
      "  \"inputRowsPerSecond\" : 2.0,\n",
      "  \"processedRowsPerSecond\" : 5.083884087442806,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 3846,\n",
      "    \"getBatch\" : 0,\n",
      "    \"getEndOffset\" : 0,\n",
      "    \"queryPlanning\" : 26,\n",
      "    \"setOffsetRange\" : 0,\n",
      "    \"triggerExecution\" : 3934,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"eventTime\" : {\n",
      "    \"avg\" : \"2023-03-11T19:14:06.940Z\",\n",
      "    \"max\" : \"2023-03-12T17:28:18.380Z\",\n",
      "    \"min\" : \"2023-03-10T21:00:00.000Z\",\n",
      "    \"watermark\" : \"2023-03-12T17:27:08.380Z\"\n",
      "  },\n",
      "  \"stateOperators\" : [ {\n",
      "    \"numRowsTotal\" : 5,\n",
      "    \"numRowsUpdated\" : 5,\n",
      "    \"memoryUsedBytes\" : 81295,\n",
      "    \"customMetrics\" : {\n",
      "      \"loadedMapCacheHitCount\" : 800,\n",
      "      \"loadedMapCacheMissCount\" : 0,\n",
      "      \"stateOnCurrentVersionSizeBytes\" : 18479\n",
      "    }\n",
      "  } ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\",\n",
      "    \"startOffset\" : 5,\n",
      "    \"endOffset\" : 15,\n",
      "    \"numInputRows\" : 10,\n",
      "    \"inputRowsPerSecond\" : 1.0,\n",
      "    \"processedRowsPerSecond\" : 2.541942043721403\n",
      "  }, {\n",
      "    \"description\" : \"RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\",\n",
      "    \"startOffset\" : 5,\n",
      "    \"endOffset\" : 15,\n",
      "    \"numInputRows\" : 10,\n",
      "    \"inputRowsPerSecond\" : 1.0,\n",
      "    \"processedRowsPerSecond\" : 2.541942043721403\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@7e60d61c\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(sq_tmp.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим с использованием `window` и `watermark` в `append` режиме. В `append` режиме в синк будут записаны только завершенные окна с данными в момент `window_right_bound` + `watermark_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/state6_sdr': No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: Nonzero exit value: 1\n",
       "StackTrace:   at scala.sys.package$.error(package.scala:27)\n",
       "  at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:132)\n",
       "  at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:102)\n",
       "  at tmp_clear(<console>:42)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_clear(\"state6_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;;\n",
       "Union\n",
       ":- Aggregate [window#2034-T20000ms], [window#2034-T20000ms AS window#2028-T20000ms, count(1) AS count#2033L]\n",
       ":  +- Filter ((timestamp#2016-T20000ms >= window#2034-T20000ms.start) && (timestamp#2016-T20000ms < window#2034-T20000ms.end))\n",
       ":     +- Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 10000000) + 0) + 20000000), LongType, TimestampType)), timestamp#2016-T20000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 10000000) + 0) + 20000000), LongType, TimestampType)), timestamp#2016-T20000ms, value#2017L, ident#2024)], [window#2034-T20000ms, timestamp#2016-T20000ms, value#2017L, ident#2024]\n",
       ":        +- EventTimeWatermark timestamp#2016: timestamp, interval 20 seconds\n",
       ":           +- Project [timestamp#2016, value#2017L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(5674183923552446640))[0] AS ident#2024]\n",
       ":              +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1f6eff1a, rate, [timestamp#2016, value#2017L]\n",
       "+- Aggregate [window#2043-T30000ms], [window#2043-T30000ms AS window#2037-T30000ms, count(1) AS count#2042L]\n",
       "   +- Filter ((timestamp#2016-T30000ms >= window#2043-T30000ms.start) && (timestamp#2016-T30000ms < window#2043-T30000ms.end))\n",
       "      +- Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024)], [window#2043-T30000ms, timestamp#2016-T30000ms, value#2017L, ident#2024]\n",
       "         +- EventTimeWatermark timestamp#2016: timestamp, interval 30 seconds\n",
       "            +- Project [timestamp#2016, value#2017L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(5674183923552446640))[0] AS ident#2024]\n",
       "               +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1f6eff1a, rate, [timestamp#2016, value#2017L]\n",
       "\n",
       "StackTrace: Union\n",
       ":- Aggregate [window#2034-T20000ms], [window#2034-T20000ms AS window#2028-T20000ms, count(1) AS count#2033L]\n",
       ":  +- Filter ((timestamp#2016-T20000ms >= window#2034-T20000ms.start) && (timestamp#2016-T20000ms < window#2034-T20000ms.end))\n",
       ":     +- Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(2 as bigint)) * 10000000) + 0) + 20000000), LongType, TimestampType)), timestamp#2016-T20000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T20000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(2 as bigint)) * 10000000) + 0) + 20000000), LongType, TimestampType)), timestamp#2016-T20000ms, value#2017L, ident#2024)], [window#2034-T20000ms, timestamp#2016-T20000ms, value#2017L, ident#2024]\n",
       ":        +- EventTimeWatermark timestamp#2016: timestamp, interval 20 seconds\n",
       ":           +- Project [timestamp#2016, value#2017L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(5674183923552446640))[0] AS ident#2024]\n",
       ":              +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1f6eff1a, rate, [timestamp#2016, value#2017L]\n",
       "+- Aggregate [window#2043-T30000ms], [window#2043-T30000ms AS window#2037-T30000ms, count(1) AS count#2042L]\n",
       "   +- Filter ((timestamp#2016-T30000ms >= window#2043-T30000ms.start) && (timestamp#2016-T30000ms < window#2043-T30000ms.end))\n",
       "      +- Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(1 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2016-T30000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(2 as bigint)) - cast(3 as bigint)) * 10000000) + 0) + 30000000), LongType, TimestampType)), timestamp#2016-T30000ms, value#2017L, ident#2024)], [window#2043-T30000ms, timestamp#2016-T30000ms, value#2017L, ident#2024]\n",
       "         +- EventTimeWatermark timestamp#2016: timestamp, interval 30 seconds\n",
       "            +- Project [timestamp#2016, value#2017L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(5674183923552446640))[0] AS ident#2024]\n",
       "               +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@1f6eff1a, rate, [timestamp#2016, value#2017L]\n",
       "  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n",
       "  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:93)\n",
       "  at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n",
       "  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n",
       "  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newData = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "\n",
    "val uData = newData\n",
    "    .withWatermark(\"timestamp\", \"20 seconds\")\n",
    "    .groupBy(window($\"timestamp\", \"20 seconds\", \"10 seconds\"))\n",
    "    .count\n",
    "\n",
    "val uData2 = newData\n",
    "    .withWatermark(\"timestamp\", \"30 seconds\")\n",
    "    .groupBy(window($\"timestamp\", \"30 seconds\", \"10 seconds\"))\n",
    "    .count\n",
    "\n",
    "val uDataAll = uData.union( uData2 )\n",
    "\n",
    "createConsoleSinkMode(\"state6_sdr\", \"append\", uDataAll).start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Spark позволяет строить агрегаты на SDF в разных режимах\n",
    "- Вотермарки поддерживаются при использовании `append` и `update` режимов\n",
    "- Плавающее окно имеет два параметра - размер окна и сдвиг текущего окна относительно следующего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.streams.active.foreach { x => x.stop }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@2251dfe0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Соединения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark позволяет делать:\n",
    "- stream - static join\n",
    "  + inner\n",
    "  + left outer\n",
    "  + left anti\n",
    "- stream - stream join\n",
    "  + inner\n",
    "  + left outer\n",
    "  + right outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream-Static join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может использоваться для:\n",
    "- обогащения стрима фактами (left outer)\n",
    "- фильтрации по blacklist (left anti)\n",
    "- фильтрации по whitelist (inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident_rate = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ident_rate = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('ident, None), unresolvedalias('name, None), unresolvedalias('elevation_ft, None), unresolvedalias('iso_country, None)]\n",
      "+- Project [ident#2058, timestamp#2050, value#2051L, type#16, name#17, elevation_ft#18, continent#19, iso_country#20, iso_region#21, municipality#22, gps_code#23, iata_code#24, local_code#25, coordinates#26]\n",
      "   +- Join LeftOuter, (ident#2058 = ident#15)\n",
      "      :- Project [timestamp#2050, value#2051L, shuffle(array(SSMK, SSML, SSMM, SSMN, SSMP), Some(-5444629639441506601))[0] AS ident#2058]\n",
      "      :  +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47992e23, rate, [timestamp#2050, value#2051L]\n",
      "      +- Relation[ident#15,type#16,name#17,elevation_ft#18,continent#19,iso_country#20,iso_region#21,municipality#22,gps_code#23,iata_code#24,local_code#25,coordinates#26] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, name: string, elevation_ft: int, iso_country: string\n",
      "Project [ident#2058, name#17, elevation_ft#18, iso_country#20]\n",
      "+- Project [ident#2058, timestamp#2050, value#2051L, type#16, name#17, elevation_ft#18, continent#19, iso_country#20, iso_region#21, municipality#22, gps_code#23, iata_code#24, local_code#25, coordinates#26]\n",
      "   +- Join LeftOuter, (ident#2058 = ident#15)\n",
      "      :- Project [timestamp#2050, value#2051L, shuffle(array(SSMK, SSML, SSMM, SSMN, SSMP), Some(-5444629639441506601))[0] AS ident#2058]\n",
      "      :  +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47992e23, rate, [timestamp#2050, value#2051L]\n",
      "      +- Relation[ident#15,type#16,name#17,elevation_ft#18,continent#19,iso_country#20,iso_region#21,municipality#22,gps_code#23,iata_code#24,local_code#25,coordinates#26] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ident#2058, name#17, elevation_ft#18, iso_country#20]\n",
      "+- Join LeftOuter, (ident#2058 = ident#15)\n",
      "   :- Project [shuffle([SSMK,SSML,SSMM,SSMN,SSMP], Some(-1599150665020609613))[0] AS ident#2058]\n",
      "   :  +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@47992e23, rate, [timestamp#2050, value#2051L]\n",
      "   +- Project [ident#15, name#17, elevation_ft#18, iso_country#20]\n",
      "      +- Filter isnotnull(ident#15)\n",
      "         +- Relation[ident#15,type#16,name#17,elevation_ft#18,continent#19,iso_country#20,iso_region#21,municipality#22,gps_code#23,iata_code#24,local_code#25,coordinates#26] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [ident#2058, name#17, elevation_ft#18, iso_country#20]\n",
      "+- *(2) BroadcastHashJoin [ident#2058], [ident#15], LeftOuter, BuildRight\n",
      "   :- *(2) Project [shuffle([SSMK,SSML,SSMM,SSMN,SSMP], Some(-1599150665020609613))[0] AS ident#2058]\n",
      "   :  +- StreamingRelation rate, [timestamp#2050, value#2051L]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(1) Project [ident#15, name#17, elevation_ft#18, iso_country#20]\n",
      "         +- *(1) Filter isnotnull(ident#15)\n",
      "            +- *(1) FileScan csv [ident#15,name#17,elevation_ft#18,iso_country#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ident)], ReadSchema: struct<ident:string,name:string,elevation_ft:int,iso_country:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "right = [ident: string, type: string ... 10 more fields]\n",
       "result = [ident: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4d8e32ae"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+----------------------------+------------+-----------+\n",
      "|ident|name                        |elevation_ft|iso_country|\n",
      "+-----+----------------------------+------------+-----------+\n",
      "|SSMN |Kururuzinho Airport         |1640        |BR         |\n",
      "|SSMK |El Dorado Airport           |20          |BR         |\n",
      "|SSMK |El Dorado Airport           |20          |BR         |\n",
      "|SSMK |El Dorado Airport           |20          |BR         |\n",
      "|SSMN |Kururuzinho Airport         |1640        |BR         |\n",
      "|SSMP |Fazenda TrÃªs Minas Airport |1391        |BR         |\n",
      "|SSMM |MACAÃ Heliport             |1247        |BR         |\n",
      "|SSMP |Fazenda TrÃªs Minas Airport |1391        |BR         |\n",
      "|SSML |Fazenda Maria LuÃ­za Airport|1083        |BR         |\n",
      "|SSMP |Fazenda TrÃªs Minas Airport |1391        |BR         |\n",
      "|SSMP |Fazenda TrÃªs Minas Airport |1391        |BR         |\n",
      "|SSMM |MACAÃ Heliport             |1247        |BR         |\n",
      "|SSMM |MACAÃ Heliport             |1247        |BR         |\n",
      "|SSML |Fazenda Maria LuÃ­za Airport|1083        |BR         |\n",
      "|SSMN |Kururuzinho Airport         |1640        |BR         |\n",
      "|SSMK |El Dorado Airport           |20          |BR         |\n",
      "|SSMP |Fazenda TrÃªs Minas Airport |1391        |BR         |\n",
      "|SSMN |Kururuzinho Airport         |1640        |BR         |\n",
      "|SSMM |MACAÃ Heliport             |1247        |BR         |\n",
      "|SSMK |El Dorado Airport           |20          |BR         |\n",
      "+-----+----------------------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+---------------+------------+-----------+\n",
      "|ident|name           |elevation_ft|iso_country|\n",
      "+-----+---------------+------------+-----------+\n",
      "|SSMM |MACAÃ Heliport|1247        |BR         |\n",
      "+-----+---------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val right = airports\n",
    "\n",
    "val result = ident_rate\n",
    "    .join(right\n",
    "         , Seq(\"ident\")\n",
    "         , \"left\")\n",
    "    .select( 'ident\n",
    "            , 'name\n",
    "            , 'elevation_ft\n",
    "            , 'iso_country)\n",
    "\n",
    "result.printSchema\n",
    "result.explain(true)\n",
    "\n",
    "createConsoleSinkMode(\"state7_sdr\", \"append\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:28:59 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state8_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state8_sdr1678642139725\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state8_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|ident|\n",
      "+-----+\n",
      "| SSMN|\n",
      "| SSMP|\n",
      "|  00A|\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----+\n",
      "|ident|timestamp              |value|\n",
      "+-----+-----------------------+-----+\n",
      "|SSMP |2023-03-12 20:29:23.287|23   |\n",
      "|SSMN |2023-03-12 20:29:25.287|25   |\n",
      "|SSMN |2023-03-12 20:29:33.287|33   |\n",
      "|SSMN |2023-03-12 20:29:37.287|37   |\n",
      "|SSMN |2023-03-12 20:29:47.287|47   |\n",
      "|SSMP |2023-03-12 20:29:49.287|49   |\n",
      "|SSMN |2023-03-12 20:29:51.287|51   |\n",
      "|SSMN |2023-03-12 20:29:53.287|53   |\n",
      "|SSMP |2023-03-12 20:30:03.287|63   |\n",
      "|SSMP |2023-03-12 20:30:07.287|67   |\n",
      "|SSMP |2023-03-12 20:30:09.287|69   |\n",
      "|SSMP |2023-03-12 20:30:11.287|71   |\n",
      "|SSMP |2023-03-12 20:30:15.287|75   |\n",
      "|SSMN |2023-03-12 20:30:19.287|79   |\n",
      "|SSMP |2023-03-12 20:30:23.287|83   |\n",
      "|SSMN |2023-03-12 20:30:25.287|85   |\n",
      "|SSMP |2023-03-12 20:30:29.287|89   |\n",
      "|SSMP |2023-03-12 20:30:33.287|93   |\n",
      "|SSMP |2023-03-12 20:30:39.287|99   |\n",
      "|SSMN |2023-03-12 20:30:41.287|101  |\n",
      "+-----+-----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "right = [ident: string]\n",
       "result = [ident: string, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@494c3bf"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+---------+-----+\n",
      "|ident|timestamp|value|\n",
      "+-----+---------+-----+\n",
      "+-----+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----+\n",
      "|ident|timestamp              |value|\n",
      "+-----+-----------------------+-----+\n",
      "|SSMN |2023-03-12 20:30:51.287|111  |\n",
      "|SSMN |2023-03-12 20:30:57.287|117  |\n",
      "+-----+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val right = Vector(\"SSMN\", \"SSMP\", \"00A\").toDF.withColumnRenamed(\"value\", \"ident\")\n",
    "right.show\n",
    "\n",
    "val result = ident_rate.join(right\n",
    "                             , Seq(\"ident\")\n",
    "                             , \"inner\")\n",
    "\n",
    "createConsoleSinkMode(\"state8_sdr\", \"append\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:31:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state9_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state9_sdr\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state9_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---------+-----+\n",
      "|ident|timestamp|value|\n",
      "+-----+---------+-----+\n",
      "+-----+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "right = [ident: string]\n",
       "result = [ident: string, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@717f6d3f"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----+\n",
      "|ident|timestamp              |value|\n",
      "+-----+-----------------------+-----+\n",
      "|SSML |2023-03-12 20:31:14.236|0    |\n",
      "|SSMP |2023-03-12 20:31:16.236|2    |\n",
      "|SSML |2023-03-12 20:31:18.236|4    |\n",
      "|SSMP |2023-03-12 20:31:15.236|1    |\n",
      "+-----+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----+\n",
      "|ident|timestamp              |value|\n",
      "+-----+-----------------------+-----+\n",
      "|SSMK |2023-03-12 20:31:19.236|5    |\n",
      "|SSML |2023-03-12 20:31:21.236|7    |\n",
      "|SSMP |2023-03-12 20:31:23.236|9    |\n",
      "|SSMP |2023-03-12 20:31:25.236|11   |\n",
      "|SSML |2023-03-12 20:31:27.236|13   |\n",
      "|SSMK |2023-03-12 20:31:20.236|6    |\n",
      "|SSMP |2023-03-12 20:31:22.236|8    |\n",
      "|SSMP |2023-03-12 20:31:24.236|10   |\n",
      "|SSMM |2023-03-12 20:31:26.236|12   |\n",
      "|SSMM |2023-03-12 20:31:28.236|14   |\n",
      "+-----+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val right = Vector( \"SSMN\", \"00A\", \"00FD\").toDF.withColumnRenamed(\"value\", \"ident\")\n",
    "val result = ident_rate.join(right, Seq(\"ident\"), \"left_anti\")\n",
    "\n",
    "createConsoleSinkMode(\"state9_sdr\", \"append\", result).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ident#2058, timestamp#2050, value#2051L]\n",
      "+- *(1) BroadcastHashJoin [ident#2058], [ident#2292], LeftAnti, BuildRight\n",
      "   :- *(1) Project [timestamp#2050, value#2051L, shuffle([SSMK,SSML,SSMM,SSMN,SSMP], Some(-5435056235164864367))[0] AS ident#2058]\n",
      "   :  +- StreamingRelation rate, [timestamp#2050, value#2051L]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- LocalTableScan [ident#2292]\n"
     ]
    }
   ],
   "source": [
    "ident_rate.join(right, Seq(\"ident\"), \"left_anti\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream-Stream join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для соединения двух стримов нам необходимо добавить к условию соединения равенство двух окон или сравнение двух временных меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('left.window, None), unresolvedalias('right.window, None), unresolvedalias('left.value, None), unresolvedalias('left, None), unresolvedalias('right, None)]\n",
      "+- Join Inner, (value#2363L = value#2388L)\n",
      "   :- SubqueryAlias `left`\n",
      "   :  +- Project [timestamp#2362-T60000ms, value#2363L, ident#2370, left#2374, window#2381-T60000ms AS window#2380]\n",
      "   :     +- Filter isnotnull(timestamp#2362-T60000ms)\n",
      "   :        +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 20000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 20000000) + 0) + 20000000), LongType, TimestampType)) AS window#2381-T60000ms, timestamp#2362-T60000ms, value#2363L, ident#2370, left#2374]\n",
      "   :           +- EventTimeWatermark timestamp#2362: timestamp, interval 1 minutes\n",
      "   :              +- Project [timestamp#2362, value#2363L, ident#2370, left AS left#2374]\n",
      "   :                 +- Project [timestamp#2362, value#2363L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-6528129553562111265))[0] AS ident#2370]\n",
      "   :                    +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@30749f06, rate, [timestamp#2362, value#2363L]\n",
      "   +- SubqueryAlias `right`\n",
      "      +- Project [timestamp#2387-T120000ms, value#2388L, ident#2395, right#2399, window#2406-T120000ms AS window#2405]\n",
      "         +- Filter isnotnull(timestamp#2387-T120000ms)\n",
      "            +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 10000000) + 0) + 10000000), LongType, TimestampType)) AS window#2406-T120000ms, timestamp#2387-T120000ms, value#2388L, ident#2395, right#2399]\n",
      "               +- EventTimeWatermark timestamp#2387: timestamp, interval 2 minutes\n",
      "                  +- Project [timestamp#2387, value#2388L, ident#2395, right AS right#2399]\n",
      "                     +- Project [timestamp#2387, value#2388L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-5425802961945940417))[0] AS ident#2395]\n",
      "                        +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@70715ad6, rate, [timestamp#2387, value#2388L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "window: struct<start:timestamp,end:timestamp>, window: struct<start:timestamp,end:timestamp>, value: bigint, left: string, right: string\n",
      "Project [window#2380, window#2405, value#2363L, left#2374, right#2399]\n",
      "+- Join Inner, (value#2363L = value#2388L)\n",
      "   :- SubqueryAlias `left`\n",
      "   :  +- Project [timestamp#2362-T60000ms, value#2363L, ident#2370, left#2374, window#2381-T60000ms AS window#2380]\n",
      "   :     +- Filter isnotnull(timestamp#2362-T60000ms)\n",
      "   :        +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 20000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / cast(20000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 20000000) + 0) + 20000000), LongType, TimestampType)) AS window#2381-T60000ms, timestamp#2362-T60000ms, value#2363L, ident#2370, left#2374]\n",
      "   :           +- EventTimeWatermark timestamp#2362: timestamp, interval 1 minutes\n",
      "   :              +- Project [timestamp#2362, value#2363L, ident#2370, left AS left#2374]\n",
      "   :                 +- Project [timestamp#2362, value#2363L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-6528129553562111265))[0] AS ident#2370]\n",
      "   :                    +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@30749f06, rate, [timestamp#2362, value#2363L]\n",
      "   +- SubqueryAlias `right`\n",
      "      +- Project [timestamp#2387-T120000ms, value#2388L, ident#2395, right#2399, window#2406-T120000ms AS window#2405]\n",
      "         +- Filter isnotnull(timestamp#2387-T120000ms)\n",
      "            +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / cast(10000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 10000000) + 0) + 10000000), LongType, TimestampType)) AS window#2406-T120000ms, timestamp#2387-T120000ms, value#2388L, ident#2395, right#2399]\n",
      "               +- EventTimeWatermark timestamp#2387: timestamp, interval 2 minutes\n",
      "                  +- Project [timestamp#2387, value#2388L, ident#2395, right AS right#2399]\n",
      "                     +- Project [timestamp#2387, value#2388L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-5425802961945940417))[0] AS ident#2395]\n",
      "                        +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@70715ad6, rate, [timestamp#2387, value#2388L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [window#2380, window#2405, value#2363L, left#2374, right#2399]\n",
      "+- Join Inner, (value#2363L = value#2388L)\n",
      "   :- Project [value#2363L, left#2374, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) END + 0) - 1) * 20000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) END + 0) - 1) * 20000000) + 20000000), LongType, TimestampType)) AS window#2380]\n",
      "   :  +- Filter isnotnull(timestamp#2362-T60000ms)\n",
      "   :     +- EventTimeWatermark timestamp#2362: timestamp, interval 1 minutes\n",
      "   :        +- Project [timestamp#2362, value#2363L, left AS left#2374]\n",
      "   :           +- Filter isnotnull(value#2363L)\n",
      "   :              +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@30749f06, rate, [timestamp#2362, value#2363L]\n",
      "   +- Project [value#2388L, right#2399, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)) AS window#2405]\n",
      "      +- Filter isnotnull(timestamp#2387-T120000ms)\n",
      "         +- EventTimeWatermark timestamp#2387: timestamp, interval 2 minutes\n",
      "            +- Project [timestamp#2387, value#2388L, right AS right#2399]\n",
      "               +- Filter isnotnull(value#2388L)\n",
      "                  +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@70715ad6, rate, [timestamp#2387, value#2388L]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(5) Project [window#2380, window#2405, value#2363L, left#2374, right#2399]\n",
      "+- StreamingSymmetricHashJoin [value#2363L], [value#2388L], Inner, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = <unknown>, runId = 228dc41e-19f4-49ff-9672-f27e83b44dbb, opId = 0, ver = 0, numPartitions = 200], 0, state cleanup [ left = null, right = null ]\n",
      "   :- Exchange hashpartitioning(value#2363L, 200)\n",
      "   :  +- *(2) Project [value#2363L, left#2374, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) END + 0) - 1) * 20000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2362-T60000ms, TimestampType, LongType) - 0) as double) / 2.0E7)) END + 0) - 1) * 20000000) + 20000000), LongType, TimestampType)) AS window#2380]\n",
      "   :     +- *(2) Filter isnotnull(timestamp#2362-T60000ms)\n",
      "   :        +- EventTimeWatermark timestamp#2362: timestamp, interval 1 minutes\n",
      "   :           +- *(1) Project [timestamp#2362, value#2363L, left AS left#2374]\n",
      "   :              +- *(1) Filter isnotnull(value#2363L)\n",
      "   :                 +- StreamingRelation rate, [timestamp#2362, value#2363L]\n",
      "   +- Exchange hashpartitioning(value#2388L, 200)\n",
      "      +- *(4) Project [value#2388L, right#2399, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#2387-T120000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)) AS window#2405]\n",
      "         +- *(4) Filter isnotnull(timestamp#2387-T120000ms)\n",
      "            +- EventTimeWatermark timestamp#2387: timestamp, interval 2 minutes\n",
      "               +- *(3) Project [timestamp#2387, value#2388L, right AS right#2399]\n",
      "                  +- *(3) Filter isnotnull(value#2388L)\n",
      "                     +- StreamingRelation rate, [timestamp#2387, value#2388L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "left = [timestamp: timestamp, value: bigint ... 3 more fields]\n",
       "right = [timestamp: timestamp, value: bigint ... 3 more fields]\n",
       "joinExpr = (left.value = right.value)\n",
       "joined = [window: struct<start: timestamp, end: timestamp>, window: struct<start: timestamp, end: timestamp> ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[window: struct<start: timestamp, end: timestamp>, window: struct<start: timestamp, end: timestamp> ... 3 more fields]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"left\", lit(\"left\"))\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\")\n",
    "    .withColumn(\"window\", window('timestamp, \"20 second\")).as(\"left\")\n",
    "\n",
    "val right = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"right\", lit(\"right\"))\n",
    "    .withWatermark(\"timestamp\", \"2 minutes\")\n",
    "    .withColumn(\"window\", window('timestamp, \"10 second\")).as(\"right\")\n",
    "\n",
    "val joinExpr = expr(\"\"\"left.value = right.value\"\"\")\n",
    "\n",
    "val joined = left\n",
    "        .join( right, joinExpr, \"inner\" )\n",
    "        .select( $\"left.window\"\n",
    "                , $\"right.window\"\n",
    "                , $\"left.value\"\n",
    "                , $\"left\"\n",
    "                , $\"right\")\n",
    "\n",
    "joined.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:31:39 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state10_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state10_sdr1678642299007\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state10_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1dd7d225"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------+-----+----+-----+\n",
      "|window|window|value|left|right|\n",
      "+------+------+-----+----+-----+\n",
      "+------+------+-----+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|window                                    |window                                    |value|left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|0    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|7    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|6    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|9    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|5    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|1    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|3    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|8    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|2    |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:40, 2023-03-12 20:31:50]|4    |left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|window                                    |window                                    |value|left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|19   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|17   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|10   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|12   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|11   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|13   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|18   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|14   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|15   |left|right|\n",
      "|[2023-03-12 20:31:40, 2023-03-12 20:32:00]|[2023-03-12 20:31:50, 2023-03-12 20:32:00]|16   |left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|window                                    |window                                    |value|left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|26   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|29   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|22   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|25   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|27   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|28   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|21   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|23   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|20   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:00, 2023-03-12 20:32:10]|24   |left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|window                                    |window                                    |value|left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|34   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|32   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|43   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|31   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|39   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|41   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|33   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|44   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|37   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|35   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|36   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|38   |left|right|\n",
      "|[2023-03-12 20:32:00, 2023-03-12 20:32:20]|[2023-03-12 20:32:10, 2023-03-12 20:32:20]|30   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|42   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|46   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|40   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|45   |left|right|\n",
      "|[2023-03-12 20:32:20, 2023-03-12 20:32:40]|[2023-03-12 20:32:20, 2023-03-12 20:32:30]|47   |left|right|\n",
      "+------------------------------------------+------------------------------------------+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkMode(\"state10_sdr\", \"append\", joined).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('left.value, None), unresolvedalias('left.timestamp, None), unresolvedalias('right.timestamp, None), unresolvedalias('left, None), unresolvedalias('right, None)]\n",
      "+- Join Inner, ((value#2491L = value#2508L) && (timestamp#2490-T60000ms <= cast(timestamp#2507-T120000ms + interval 1 minutes as timestamp)))\n",
      "   :- SubqueryAlias `left`\n",
      "   :  +- EventTimeWatermark timestamp#2490: timestamp, interval 1 minutes\n",
      "   :     +- Project [timestamp#2490, value#2491L, ident#2498, left AS left#2502]\n",
      "   :        +- Project [timestamp#2490, value#2491L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-9065781549738897253))[0] AS ident#2498]\n",
      "   :           +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4fef5c6a, rate, [timestamp#2490, value#2491L]\n",
      "   +- SubqueryAlias `right`\n",
      "      +- EventTimeWatermark timestamp#2507: timestamp, interval 2 minutes\n",
      "         +- Project [timestamp#2507, value#2508L, ident#2515, right AS right#2519]\n",
      "            +- Project [timestamp#2507, value#2508L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(650958260398801237))[0] AS ident#2515]\n",
      "               +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4a2bab3f, rate, [timestamp#2507, value#2508L]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "value: bigint, timestamp: timestamp, timestamp: timestamp, left: string, right: string\n",
      "Project [value#2491L, timestamp#2490-T60000ms, timestamp#2507-T120000ms, left#2502, right#2519]\n",
      "+- Join Inner, ((value#2491L = value#2508L) && (timestamp#2490-T60000ms <= cast(timestamp#2507-T120000ms + interval 1 minutes as timestamp)))\n",
      "   :- SubqueryAlias `left`\n",
      "   :  +- EventTimeWatermark timestamp#2490: timestamp, interval 1 minutes\n",
      "   :     +- Project [timestamp#2490, value#2491L, ident#2498, left AS left#2502]\n",
      "   :        +- Project [timestamp#2490, value#2491L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(-9065781549738897253))[0] AS ident#2498]\n",
      "   :           +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4fef5c6a, rate, [timestamp#2490, value#2491L]\n",
      "   +- SubqueryAlias `right`\n",
      "      +- EventTimeWatermark timestamp#2507: timestamp, interval 2 minutes\n",
      "         +- Project [timestamp#2507, value#2508L, ident#2515, right AS right#2519]\n",
      "            +- Project [timestamp#2507, value#2508L, shuffle(array(00A, 00AA, 00AK, 00AL, 00AR), Some(650958260398801237))[0] AS ident#2515]\n",
      "               +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4a2bab3f, rate, [timestamp#2507, value#2508L]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [value#2491L, timestamp#2490-T60000ms, timestamp#2507-T120000ms, left#2502, right#2519]\n",
      "+- Join Inner, ((value#2491L = value#2508L) && (timestamp#2490-T60000ms <= timestamp#2507-T120000ms + interval 1 minutes))\n",
      "   :- Filter isnotnull(timestamp#2490-T60000ms)\n",
      "   :  +- EventTimeWatermark timestamp#2490: timestamp, interval 1 minutes\n",
      "   :     +- Project [timestamp#2490, value#2491L, left AS left#2502]\n",
      "   :        +- Filter isnotnull(value#2491L)\n",
      "   :           +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4fef5c6a, rate, [timestamp#2490, value#2491L]\n",
      "   +- EventTimeWatermark timestamp#2507: timestamp, interval 2 minutes\n",
      "      +- Project [timestamp#2507, value#2508L, right AS right#2519]\n",
      "         +- Filter isnotnull(value#2508L)\n",
      "            +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@4a2bab3f, rate, [timestamp#2507, value#2508L]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Project [value#2491L, timestamp#2490-T60000ms, timestamp#2507-T120000ms, left#2502, right#2519]\n",
      "+- StreamingSymmetricHashJoin [value#2491L], [value#2508L], Inner, condition = [ leftOnly = null, rightOnly = null, both = (timestamp#2490-T60000ms <= timestamp#2507-T120000ms + interval 1 minutes), full = (timestamp#2490-T60000ms <= timestamp#2507-T120000ms + interval 1 minutes) ], state info [ checkpoint = <unknown>, runId = c5d57983-61cd-4604-935f-de921baef486, opId = 0, ver = 0, numPartitions = 200], 0, state cleanup [ left = null, right value predicate: (timestamp#2507-T120000ms <= -60001000) ]\n",
      "   :- Exchange hashpartitioning(value#2491L, 200)\n",
      "   :  +- *(2) Filter isnotnull(timestamp#2490-T60000ms)\n",
      "   :     +- EventTimeWatermark timestamp#2490: timestamp, interval 1 minutes\n",
      "   :        +- *(1) Project [timestamp#2490, value#2491L, left AS left#2502]\n",
      "   :           +- *(1) Filter isnotnull(value#2491L)\n",
      "   :              +- StreamingRelation rate, [timestamp#2490, value#2491L]\n",
      "   +- Exchange hashpartitioning(value#2508L, 200)\n",
      "      +- EventTimeWatermark timestamp#2507: timestamp, interval 2 minutes\n",
      "         +- *(3) Project [timestamp#2507, value#2508L, right AS right#2519]\n",
      "            +- *(3) Filter isnotnull(value#2508L)\n",
      "               +- StreamingRelation rate, [timestamp#2507, value#2508L]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "left = [timestamp: timestamp, value: bigint ... 2 more fields]\n",
       "right = [timestamp: timestamp, value: bigint ... 2 more fields]\n",
       "joinExpr = ((left.value = right.value) AND (left.timestamp <= (right.timestamp + interval 1 minutes)))\n",
       "joined = [value: bigint, timestamp: timestamp ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: bigint, timestamp: timestamp ... 3 more fields]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"left\", lit(\"left\"))\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\").as(\"left\")\n",
    "\n",
    "val right = spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .load\n",
    "    .withColumn(\"ident\", randomIdent())\n",
    "    .withColumn(\"right\", lit(\"right\"))\n",
    "    .withWatermark(\"timestamp\", \"2 minutes\").as(\"right\")\n",
    "\n",
    "val joinExpr = expr(\"\"\"left.value = right.value and left.timestamp <= right.timestamp + INTERVAL 1 minutes \"\"\")\n",
    "\n",
    "val joined = left.join(right, joinExpr, \"inner\").select($\"left.value\", $\"left.timestamp\", $\"right.timestamp\", $\"left\", $\"right\")\n",
    "\n",
    "joined.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/12 20:32:05 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-master-1.newprolab.com:8020/tmp/state11_sdr' to trash at: hdfs://spark-master-1.newprolab.com:8020/user/dinar.sadykov/.Trash/Current/tmp/state11_sdr\n"
     ]
    }
   ],
   "source": [
    "tmp_clear(\"state11_sdr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@388706d5"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---------+---------+----+-----+\n",
      "|value|timestamp|timestamp|left|right|\n",
      "+-----+---------+---------+----+-----+\n",
      "+-----+---------+---------+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "|value|timestamp              |timestamp              |left|right|\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "|0    |2023-03-12 20:32:05.664|2023-03-12 20:32:05.685|left|right|\n",
      "|7    |2023-03-12 20:32:12.664|2023-03-12 20:32:12.685|left|right|\n",
      "|6    |2023-03-12 20:32:11.664|2023-03-12 20:32:11.685|left|right|\n",
      "|9    |2023-03-12 20:32:14.664|2023-03-12 20:32:14.685|left|right|\n",
      "|5    |2023-03-12 20:32:10.664|2023-03-12 20:32:10.685|left|right|\n",
      "|1    |2023-03-12 20:32:06.664|2023-03-12 20:32:06.685|left|right|\n",
      "|10   |2023-03-12 20:32:15.664|2023-03-12 20:32:15.685|left|right|\n",
      "|3    |2023-03-12 20:32:08.664|2023-03-12 20:32:08.685|left|right|\n",
      "|12   |2023-03-12 20:32:17.664|2023-03-12 20:32:17.685|left|right|\n",
      "|8    |2023-03-12 20:32:13.664|2023-03-12 20:32:13.685|left|right|\n",
      "|11   |2023-03-12 20:32:16.664|2023-03-12 20:32:16.685|left|right|\n",
      "|2    |2023-03-12 20:32:07.664|2023-03-12 20:32:07.685|left|right|\n",
      "|4    |2023-03-12 20:32:09.664|2023-03-12 20:32:09.685|left|right|\n",
      "|13   |2023-03-12 20:32:18.664|2023-03-12 20:32:18.685|left|right|\n",
      "|14   |2023-03-12 20:32:19.664|2023-03-12 20:32:19.685|left|right|\n",
      "|15   |2023-03-12 20:32:20.664|2023-03-12 20:32:20.685|left|right|\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "|value|timestamp              |timestamp              |left|right|\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "|26   |2023-03-12 20:32:31.664|2023-03-12 20:32:31.685|left|right|\n",
      "|29   |2023-03-12 20:32:34.664|2023-03-12 20:32:34.685|left|right|\n",
      "|19   |2023-03-12 20:32:24.664|2023-03-12 20:32:24.685|left|right|\n",
      "|22   |2023-03-12 20:32:27.664|2023-03-12 20:32:27.685|left|right|\n",
      "|32   |2023-03-12 20:32:37.664|2023-03-12 20:32:37.685|left|right|\n",
      "|31   |2023-03-12 20:32:36.664|2023-03-12 20:32:36.685|left|right|\n",
      "|25   |2023-03-12 20:32:30.664|2023-03-12 20:32:30.685|left|right|\n",
      "|27   |2023-03-12 20:32:32.664|2023-03-12 20:32:32.685|left|right|\n",
      "|17   |2023-03-12 20:32:22.664|2023-03-12 20:32:22.685|left|right|\n",
      "|28   |2023-03-12 20:32:33.664|2023-03-12 20:32:33.685|left|right|\n",
      "|33   |2023-03-12 20:32:38.664|2023-03-12 20:32:38.685|left|right|\n",
      "|18   |2023-03-12 20:32:23.664|2023-03-12 20:32:23.685|left|right|\n",
      "|21   |2023-03-12 20:32:26.664|2023-03-12 20:32:26.685|left|right|\n",
      "|30   |2023-03-12 20:32:35.664|2023-03-12 20:32:35.685|left|right|\n",
      "|23   |2023-03-12 20:32:28.664|2023-03-12 20:32:28.685|left|right|\n",
      "|20   |2023-03-12 20:32:25.664|2023-03-12 20:32:25.685|left|right|\n",
      "|16   |2023-03-12 20:32:21.664|2023-03-12 20:32:21.685|left|right|\n",
      "|24   |2023-03-12 20:32:29.664|2023-03-12 20:32:29.685|left|right|\n",
      "+-----+-----------------------+-----------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createConsoleSinkMode(\"state11_sdr\", \"append\", joined).start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n",
      "Stopped RateStreamV2[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=default\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет соединять SDF со статическим DF, используя разные виды соединений: left outer, inner, left anti\n",
    "- Допускается использование соединений двух стримов, для этого требуется использовать вотермарки и (опционально) плавающие окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
